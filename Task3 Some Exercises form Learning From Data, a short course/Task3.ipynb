{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4db57b61",
   "metadata": {},
   "source": [
    "# Some Exercises Chapter one (Learning from Data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef3a204",
   "metadata": {},
   "source": [
    "## 1.2\n",
    "Suppose that we use a perceptron to detect spam messages. Let's say that each email message is represented by the frequency of occurrence of keywords, and the output is if the message is considered spam."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48ff5d5",
   "metadata": {},
   "source": [
    "#### (a) Can you thinkofsome keywords that will end up with a large positive weight in the perceptron?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989a43a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Keywords with a large positive weight: , congrats, free, cheap, earn, Oportunity, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2326eb",
   "metadata": {},
   "source": [
    "#### (b) How about keywords that will get a negative weight?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98c4c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    " person name, hi, the, he, she, regards, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481dc523",
   "metadata": {},
   "source": [
    "#### (c) What parameter in the perceptron directly affects how many borderline messages end up being classified as spam?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaab6bce",
   "metadata": {},
   "source": [
    "The parameter  in perceptron directly affects how many borderline messages end up being classified as spam. This is because  is the threshold used to classify the emails into spam and non-spam categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76543fce",
   "metadata": {},
   "source": [
    "## 1.3 \n",
    "The weight update rule in (1.3) has the nice interpretation that it moves in the direction of classifying $x(t)$ correctly. \n",
    "\n",
    "$w(t+1)= w(t)+y(t)x(t)$. (1.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a897b262",
   "metadata": {},
   "source": [
    "### (a) Show that $y(t)w^{T}(t)x(t) < 0$. [Hint: $x(t)$ is misclassified by $w(t)$.]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675b2011",
   "metadata": {},
   "source": [
    "Note that if $x(t)$ is misclassified by $w(t)$, then $w^{T}(t)x(t)$\n",
    " has different signs of $y(t)$ , thus $y(t)w^{T}(t)x(t) > 0$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d17642",
   "metadata": {},
   "source": [
    "### (b)  Show that $y(t)w^{T}(t+1)x(t) > y(t)w^{T}(t)x(t)$. [Hint: Use (1.3).]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a37e7f2",
   "metadata": {},
   "source": [
    "Note the next relation: $$y(t)w^{T}(t)x(t) = y(t)(w(t)+ y(t)x(t))^{T}x(t)=y(t)(w(t)^{T}+ y(t)x(t)^{T})x(t)= y(t)w^{T}(t)x(t) + y(t)y(t)x(t)^{T}x(t)>y(t)w(t)^{T}x(t)$$, by the (a) part (1.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788edecb",
   "metadata": {},
   "source": [
    "### (c) As far as classifying $x(t)$ is concerned, argue that the move from $w(t)$ to $w(t+ 1)$ is a move 'in the right direction'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d8ee3f",
   "metadata": {},
   "source": [
    "From (b) part, we  have  $y(t)w^{T}(t)x(t)$ is increasing with each iteration. Now, if $y(t)>0$, but $w^{T}(t)x(t)<0$, we move $w^{T}(t)x(t)$\n",
    " toward positive by increasing it. In the similar way when $y(t)<0$.\n",
    " So the move $w(t)$ from  to $w(t+1)$ is a move \"in the right direction\" as far as classifying  is $x(t)$ concerned.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e8387d",
   "metadata": {},
   "source": [
    "## 1.10 \n",
    "Here is an experiment that illustrates the difference between a single bin and multiple bins. Run a computer simulation for flipping 1,000 fair coins. Flip each coin independently times. Let's focus on 3 coins as follows: $c_1$ is the first coin flipped; $c_{rand}$ is a coin you choose at random; $c_{min}$ is the coin that had the minimum frequency of heads (pick the earlier one in case of a tie). Let $v_1$ , $v_{rand}$ and $v_{min}$ be the fraction of heads you obtain for the respective three coins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe4ea0d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "ArgumentError: Package numpy not found in current path:\n- Run `import Pkg; Pkg.add(\"numpy\")` to install the numpy package.\n",
     "output_type": "error",
     "traceback": [
      "ArgumentError: Package numpy not found in current path:\n- Run `import Pkg; Pkg.add(\"numpy\")` to install the numpy package.\n",
      "",
      "Stacktrace:",
      " [1] require(into::Module, mod::Symbol)",
      "   @ Base ./loading.jl:967",
      " [2] eval",
      "   @ ./boot.jl:373 [inlined]",
      " [3] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base ./loading.jl:1196"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "import math\n",
    "from sklearn.preprocessing import normalize\n",
    "from functools import partial\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "def flip_coins(total_coins):\n",
    "    \"\"\"Flip all coins once, return their head/tail status\n",
    "    \"\"\"\n",
    "    \n",
    "    hts = np.zeros(total_coins) #head: 1, tail: 0\n",
    "    probs = np.random.uniform(size=total_coins)\n",
    "    hts[probs > 0.5] = 1\n",
    "    return hts\n",
    "\n",
    "def run_once(total_coins, total_flips, print_freq = False):\n",
    "    v1, vrand, vmin = None, None, None\n",
    "    crand = np.random.choice(total_coins)\n",
    "    hts_sum = np.zeros(total_coins) # store the sum of heads in total_flips\n",
    "    \n",
    "    for flip in range(total_flips):\n",
    "        hts_sum = hts_sum + flip_coins(total_coins)\n",
    "    \n",
    "    hts_freq = hts_sum/total_flips\n",
    "    \n",
    "    v1 = hts_freq[0]\n",
    "    vrand = hts_freq[crand]\n",
    "    cmin = np.argmin(hts_sum)\n",
    "    vmin = hts_freq[cmin]\n",
    "    \n",
    "    if print_freq:\n",
    "        print('Frequency of first coin: {}'.format(v1))\n",
    "        print('Frequency of a random coin: id({})-freq({})'.format(crand, vrand))\n",
    "        print('Frequency of the coin with minimum frequency: id({})-freq({})'.format(cmin, vmin))\n",
    "    return v1,vrand,vmin\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47ad41b",
   "metadata": {},
   "source": [
    "### (a) What is $μ$ for the three coins selected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca61ef8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e4239cac",
   "metadata": {},
   "source": [
    "### (b) \n",
    "Repeat this entire experiment a large number of times (e.g., 100, 000 runs of the entire experiment) to get several instances of $v_1$ , $v_{rand}$ and $v_{min}$ and plot the histograms of the distributions of $v_1$ , $v_{rand}$ and $v_{min}$. Notice that which coins end up being $c_{rand}$ and $c_{min}$ may differ from one run to another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f317300",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "48e85ad8",
   "metadata": {},
   "source": [
    "### (c) Using (b), plot estimates for $\\mathbb{P}[|v-μ| > \\epsilon]$ as a function of $\\epsilon$, together with the Hoeffding bound $2e^{-2\\epsilon^{2}N}$ (on the same graph)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40130be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "136d87c8",
   "metadata": {},
   "source": [
    "### (d) Which coins obey the Hoeffding bound, and which ones do not? Ex­ plain why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a5e8fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "26e0a568",
   "metadata": {},
   "source": [
    "### (e) Relate part (d) to the multiple bins in Figure 1.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8cc26f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4c89882f",
   "metadata": {},
   "source": [
    "## 1.11 \n",
    "We are given a data set $D$ of 25 training examples from an unknown target function $f:X \\rightarrow Y$ , where $X=\\mathbb{R}$ and $Y =\\{-1,+1\\}$. To learn $f$, we use a simple hypothesis set $H=\\{h_1, h_2\\}$ where $h_1$ is the constant function and $h_2$ is the constant -1.We consider two learning algorithms, S (smart) and  C (crazy). S chooses the hypothesis that agrees the most with $D$ and C chooses the other hypothesis de liberately. Let us see how these algorithms perform out of sample from the deterministic and probabilistic points of view. Assume in the probabilistic view that there is a probability distribution on $X$, and let $\\mathbb{P}[f(x) = +1]= p$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31df0263",
   "metadata": {},
   "source": [
    "### (a) Can S produce a hypothesis that is guaranteed to perform better than random on any point outside $D$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8813eb1",
   "metadata": {},
   "source": [
    "$S$  can not produce a hypothesis that is guaranteed to perform better than random on any point outside $D$. If $f$ has 25+1  on $D$ but -1 on all other points in $X$, $S$ will choose the hypothesis $h_1$ \n",
    ", which will not match $f$ outside of $D$ at all. On the other hand, a random function will have +1 and -1 50/50, and it matches $f$ half of time, which is better than the function produced by $S$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feba14bf",
   "metadata": {},
   "source": [
    "### (b) Assume for the rest of the exercise that all the examples in $D$ have $Y_n = 1$. Is it possible that the hypothesis that C produces turns out to be better than the hypothesis that S produces?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e846ba1c",
   "metadata": {},
   "source": [
    "It is possible that $C$ produces a better hypothesis than $S$ produces. See the previous example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3a4353",
   "metadata": {},
   "source": [
    "### (c) If $p = 0.9$, what is the probability that S will produce a better hypothesis than C?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb55d9f",
   "metadata": {},
   "source": [
    " NOte that if every point in $D$ has 1, then $S$ will choose $h_1$ \n",
    " and $C$ will choose $h_2$\n",
    ". So outside of $D$, $h_1$\n",
    " will have 90% chance to match with $f$, while $h_2$\n",
    " will have only 10% chance. $S$ will always produce a better hypothesis than $C$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a59e6a",
   "metadata": {},
   "source": [
    "### (d) Is there any value of $p$ for which it is more likely than not that C will produce a better hypothesis than S?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10dccace",
   "metadata": {},
   "source": [
    "From previous problem, we can see that when $p<0.5$ ,$C$  will produce a better hypothesis than $S$. Since $C$ always produce \n",
    "$h_2$, which will match $f$ better than $h_1$\n",
    " if $p<0.5$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fa6f5b",
   "metadata": {},
   "source": [
    "## Exercise 1.12\n",
    "A friend comes to you with a learning problem. She says the target function $f$ is completely unknown, but she has 4, 000 data points. She is willing to pay you to solve her problem and produce for her a $g$ which approximates $f$. What is the best that you can promise her among the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4864fee4",
   "metadata": {},
   "source": [
    "### (a) After learning you will provide her with a $g$ that you will guarantee approximates $f$ well out of sample."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e03376",
   "metadata": {},
   "source": [
    "### (b) After learning you will provide her with a $g$, and with high probability the $g$ which you produce will approximate $f$ well out of sample."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f253cc45",
   "metadata": {},
   "source": [
    "###  (c) One of two things will happen.\n",
    "(i) You will produce a hypothesis $g$; \n",
    "(ii) You will declare that you failed.\n",
    "If you do return a hypothesis $g$, then with high probabilitythe $g$ which\n",
    "you produce will approximate $f$ well out of sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929593a7",
   "metadata": {},
   "source": [
    "### I think the best is (c)\n",
    "Because, If we can learn and produce a hypothesis $g$, since there are many data points (4000), the probability that $g$ matches $f$ is high according to Hoeffding inequality, and the error on $g$ might be small since we have a large data set. In the other hand, the unknown target  $f$ can be very complex that we can't learn at all."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.2",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
