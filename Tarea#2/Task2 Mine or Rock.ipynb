{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6e4532a",
   "metadata": {},
   "source": [
    "#                                               Task2                                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22db041",
   "metadata": {},
   "source": [
    "#                                              About the Dataset\n",
    "\n",
    "The Sonar Dataset involves the prediction of whether or not an object is a mine or a rock given the\n",
    "strength of sonar returns at different angles. It is a binary (2-class) classification problem. The\n",
    "number of observations for each class is not balanced. There are 208 observations with 60 input\n",
    "variables and 1 output variable. The variable names are as follows:\n",
    "Sonar returns at different angles\n",
    "…\n",
    "Class (M for mine and R for rock)\n",
    "\n",
    "# # About this Note Book\n",
    "\n",
    "### 1.\n",
    "Let's to use some algorithms to predict the label for some feateres from the sonar dataset. Split our dataset in training and testing subsets is the first step. \n",
    "In this way, we to define a funtion that it input is the labels, and the split percetege, and the output are the indeces from the training data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0324a4",
   "metadata": {},
   "source": [
    "### 2. The libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3de7c62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "using GLMNet\n",
    "using RDatasets\n",
    "using MLBase\n",
    "using Plots\n",
    "using DecisionTree\n",
    "using Distances\n",
    "using NearestNeighbors\n",
    "using Random\n",
    "using LinearAlgebra\n",
    "using DataStructures\n",
    "using LIBSVM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6284afaa",
   "metadata": {},
   "source": [
    "We will use a simple funtion for the find the accuracy in each case\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3a9443d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "findaccuracy(predictedvals,groundtruthvals) = sum(predictedvals.==groundtruthvals)/length(groundtruthvals)\n",
    "using CSV\n",
    "using DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1ef3bc",
   "metadata": {},
   "source": [
    "### 3. Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b6e412d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sonardata=CSV.read(\"sonar.all-data\", DataFrame);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "66d15feb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "207-element PooledArrays.PooledVector{String1, UInt32, Vector{UInt32}}:\n",
       " \"R\"\n",
       " \"R\"\n",
       " \"R\"\n",
       " \"R\"\n",
       " \"R\"\n",
       " \"R\"\n",
       " \"R\"\n",
       " \"R\"\n",
       " \"R\"\n",
       " \"R\"\n",
       " \"R\"\n",
       " \"R\"\n",
       " \"R\"\n",
       " ⋮\n",
       " \"M\"\n",
       " \"M\"\n",
       " \"M\"\n",
       " \"M\"\n",
       " \"M\"\n",
       " \"M\"\n",
       " \"M\"\n",
       " \"M\"\n",
       " \"M\"\n",
       " \"M\"\n",
       " \"M\"\n",
       " \"M\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets to crate a matrix for the sonar measures, and other for the clasification\n",
    "X=Matrix(sonardata[:,1:60])\n",
    "sonarlabels=sonardata[:,61] #Note that our dataset is very organized\n",
    "y=sonarlabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "61119d7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "207×60 Matrix{Float64}:\n",
       " 0.0453  0.0523  0.0843  0.0689  0.1183  …  0.014   0.0049  0.0052  0.0044\n",
       " 0.0262  0.0582  0.1099  0.1083  0.0974     0.0316  0.0164  0.0095  0.0078\n",
       " 0.01    0.0171  0.0623  0.0205  0.0205     0.005   0.0044  0.004   0.0117\n",
       " 0.0762  0.0666  0.0481  0.0394  0.059      0.0072  0.0048  0.0107  0.0094\n",
       " 0.0286  0.0453  0.0277  0.0174  0.0384     0.0057  0.0027  0.0051  0.0062\n",
       " 0.0317  0.0956  0.1321  0.1408  0.1674  …  0.0092  0.0143  0.0036  0.0103\n",
       " 0.0519  0.0548  0.0842  0.0319  0.1158     0.0085  0.0047  0.0048  0.0053\n",
       " 0.0223  0.0375  0.0484  0.0475  0.0647     0.0065  0.0093  0.0059  0.0022\n",
       " 0.0164  0.0173  0.0347  0.007   0.0187     0.0032  0.0035  0.0056  0.004\n",
       " 0.0039  0.0063  0.0152  0.0336  0.031      0.0042  0.0003  0.0053  0.0036\n",
       " 0.0123  0.0309  0.0169  0.0313  0.0358  …  0.0026  0.0092  0.0009  0.0044\n",
       " 0.0079  0.0086  0.0055  0.025   0.0344     0.0059  0.0058  0.0059  0.0032\n",
       " 0.009   0.0062  0.0253  0.0489  0.1197     0.0158  0.0053  0.0189  0.0102\n",
       " ⋮                                       ⋱                          \n",
       " 0.005   0.0017  0.027   0.045   0.0958  …  0.0024  0.0063  0.0017  0.0028\n",
       " 0.0366  0.0421  0.0504  0.025   0.0596     0.0025  0.0017  0.0027  0.0027\n",
       " 0.0238  0.0318  0.0422  0.0399  0.0788     0.0028  0.0013  0.0035  0.006\n",
       " 0.0116  0.0744  0.0367  0.0225  0.0076     0.0037  0.0044  0.0057  0.0035\n",
       " 0.0131  0.0387  0.0329  0.0078  0.0721     0.004   0.0009  0.0015  0.0085\n",
       " 0.0335  0.0258  0.0398  0.057   0.0529  …  0.0045  0.0022  0.0005  0.0031\n",
       " 0.0272  0.0378  0.0488  0.0848  0.1127     0.0054  0.0051  0.0065  0.0103\n",
       " 0.0187  0.0346  0.0168  0.0177  0.0393     0.0065  0.0115  0.0193  0.0157\n",
       " 0.0323  0.0101  0.0298  0.0564  0.076      0.0034  0.0032  0.0062  0.0067\n",
       " 0.0522  0.0437  0.018   0.0292  0.0351     0.014   0.0138  0.0077  0.0031\n",
       " 0.0303  0.0353  0.049   0.0608  0.0167  …  0.0034  0.0079  0.0036  0.0048\n",
       " 0.026   0.0363  0.0136  0.0272  0.0214     0.004   0.0036  0.0061  0.0115"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6babdd",
   "metadata": {},
   "source": [
    "# Feature Engineering: Standarization, Normalization and the composition of both."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35c9cb9",
   "metadata": {},
   "source": [
    "## Standarization : We use the StatsBase pckgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "15bcfe7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "207×60 Matrix{Float64}:\n",
       " -0.85433   -0.828486  -0.710344  …  -1.00349   -1.00238   -1.00533\n",
       " -1.16564   -1.05528   -0.876986     -1.19943   -1.22323   -1.22909\n",
       " -1.01056   -0.982588  -0.804514     -1.03262   -1.0342    -1.00386\n",
       " -0.790729  -0.830855  -0.90818      -1.08916   -1.0645    -1.06994\n",
       " -0.91719   -0.857317  -0.920417     -1.01005   -1.00144   -0.997499\n",
       " -1.00421   -0.753625  -0.610491  …  -1.07244   -1.1144    -1.08813\n",
       " -0.770732  -0.759871  -0.649766     -0.9475    -0.947125  -0.945252\n",
       " -0.825939  -0.769773  -0.729497     -0.873976  -0.886539  -0.900211\n",
       " -0.695247  -0.69219   -0.633093     -0.73906   -0.731928  -0.737362\n",
       " -0.803619  -0.795283  -0.764371     -0.816123  -0.798756  -0.804661\n",
       " -0.836681  -0.767334  -0.81953   …  -0.848238  -0.879184  -0.866134\n",
       " -0.788449  -0.785872  -0.797283     -0.796179  -0.795811  -0.80575\n",
       " -1.08069   -1.09204   -1.0146       -1.09569   -1.04055   -1.07582\n",
       "  ⋮                               ⋱                        \n",
       " -0.850352  -0.860824  -0.780538  …  -0.846226  -0.860824  -0.857333\n",
       " -0.752119  -0.733894  -0.706391     -0.867766  -0.864453  -0.864453\n",
       " -0.782835  -0.755957  -0.721015     -0.858431  -0.851039  -0.84264\n",
       " -0.825081  -0.622175  -0.743983     -0.848344  -0.844144  -0.851252\n",
       " -0.875785  -0.792501  -0.81137      -0.915476  -0.913524  -0.890751\n",
       " -0.777281  -0.803133  -0.75613   …  -0.882365  -0.888073  -0.879344\n",
       " -0.814351  -0.77607   -0.736345     -0.894164  -0.889108  -0.875385\n",
       " -0.818705  -0.762904  -0.825373     -0.843974  -0.8166    -0.829234\n",
       " -0.714258  -0.786109  -0.72235      -0.808441  -0.798731  -0.797113\n",
       " -0.626756  -0.654871  -0.739879     -0.753771  -0.773948  -0.789163\n",
       " -0.736384  -0.718801  -0.670624  …  -0.815155  -0.830276  -0.826056\n",
       " -0.767168  -0.732134  -0.809346     -0.84336   -0.834856  -0.816488"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XStandarized=X\n",
    "dt = fit(ZScoreTransform, XStandarized, dims=2)\n",
    "StatsBase.transform(dt, XStandarized)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822b45e0",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "51544820",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "207×60 Matrix{Float64}:\n",
       " 0.0410808   0.0481117    0.0802531  0.0647851   …  0.000803536  0.0\n",
       " 0.0257419   0.0617131    0.119829   0.118031       0.00696942   0.00505845\n",
       " 0.00651665  0.0137461    0.0597699  0.017208       0.00040729   0.00824763\n",
       " 0.0748122   0.0651978    0.04667    0.0379569      0.00921382   0.00791187\n",
       " 0.0273355   0.0440573    0.0264344  0.016121       0.00380495   0.00490638\n",
       " 0.0289721   0.0948551    0.132488   0.141458    …  0.0          0.00690793\n",
       " 0.0506573   0.0537565    0.0851769  0.0292829      0.000320616  0.000854975\n",
       " 0.0202969   0.0356458    0.0466525  0.0457437      0.00373624   0.0\n",
       " 0.0136265   0.0145556    0.0325178  0.00392278     0.00247755   0.000825849\n",
       " 0.00370485  0.00617475   0.015334   0.0342698      0.00514562   0.00339611\n",
       " 0.0114103   0.030027     0.0160144  0.0304274   …  0.0          0.00350315\n",
       " 0.00610501  0.00681726   0.003663   0.0235043      0.00407      0.00132275\n",
       " 0.00428389  0.00104203   0.0231562  0.0504805      0.0157462    0.00567327\n",
       " ⋮                                               ⋱               \n",
       " 0.004004    0.000700701  0.026026   0.044044    …  0.000700701  0.0018018\n",
       " 0.0350561   0.0405649    0.0488782  0.0234375      0.00110176   0.00110176\n",
       " 0.0225293   0.0305397    0.0409532  0.0386502      0.00220286   0.00470612\n",
       " 0.00902346  0.0719872    0.0341889  0.0199519      0.00310808   0.000902346\n",
       " 0.012211    0.0378341    0.0320288  0.00690622     0.00060054   0.00760685\n",
       " 0.0331658   0.0254271    0.0394975  0.0567839   …  0.0          0.00261307\n",
       " 0.0229989   0.0336447    0.0446922  0.0808476      0.0022095    0.00602591\n",
       " 0.015451    0.0314036    0.0135447  0.0144477      0.016053     0.0124411\n",
       " 0.0291934   0.00692215   0.0266854  0.0533708      0.00300963   0.00351124\n",
       " 0.0494434   0.0409187    0.0151439  0.0263765      0.00481396   0.000200582\n",
       " 0.0269918   0.0320088    0.0457556  0.0575958   …  0.000200682  0.00140478\n",
       " 0.0224809   0.0328181    0.0100361  0.0236853      0.00250903   0.00792854"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XNormalized=X\n",
    "dt = fit(UnitRangeTransform, XNormalized, dims=2)\n",
    "StatsBase.transform(dt, XNormalized)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbd4862",
   "metadata": {},
   "source": [
    "## Both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "17820c3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "207×60 Matrix{Float64}:\n",
       " -0.85433   -0.828486  -0.710344  …  -1.00349   -1.00238   -1.00533\n",
       " -1.16564   -1.05528   -0.876986     -1.19943   -1.22323   -1.22909\n",
       " -1.01056   -0.982588  -0.804514     -1.03262   -1.0342    -1.00386\n",
       " -0.790729  -0.830855  -0.90818      -1.08916   -1.0645    -1.06994\n",
       " -0.91719   -0.857317  -0.920417     -1.01005   -1.00144   -0.997499\n",
       " -1.00421   -0.753625  -0.610491  …  -1.07244   -1.1144    -1.08813\n",
       " -0.770732  -0.759871  -0.649766     -0.9475    -0.947125  -0.945252\n",
       " -0.825939  -0.769773  -0.729497     -0.873976  -0.886539  -0.900211\n",
       " -0.695247  -0.69219   -0.633093     -0.73906   -0.731928  -0.737362\n",
       " -0.803619  -0.795283  -0.764371     -0.816123  -0.798756  -0.804661\n",
       " -0.836681  -0.767334  -0.81953   …  -0.848238  -0.879184  -0.866134\n",
       " -0.788449  -0.785872  -0.797283     -0.796179  -0.795811  -0.80575\n",
       " -1.08069   -1.09204   -1.0146       -1.09569   -1.04055   -1.07582\n",
       "  ⋮                               ⋱                        \n",
       " -0.850352  -0.860824  -0.780538  …  -0.846226  -0.860824  -0.857333\n",
       " -0.752119  -0.733894  -0.706391     -0.867766  -0.864453  -0.864453\n",
       " -0.782835  -0.755957  -0.721015     -0.858431  -0.851039  -0.84264\n",
       " -0.825081  -0.622175  -0.743983     -0.848344  -0.844144  -0.851252\n",
       " -0.875785  -0.792501  -0.81137      -0.915476  -0.913524  -0.890751\n",
       " -0.777281  -0.803133  -0.75613   …  -0.882365  -0.888073  -0.879344\n",
       " -0.814351  -0.77607   -0.736345     -0.894164  -0.889108  -0.875385\n",
       " -0.818705  -0.762904  -0.825373     -0.843974  -0.8166    -0.829234\n",
       " -0.714258  -0.786109  -0.72235      -0.808441  -0.798731  -0.797113\n",
       " -0.626756  -0.654871  -0.739879     -0.753771  -0.773948  -0.789163\n",
       " -0.736384  -0.718801  -0.670624  …  -0.815155  -0.830276  -0.826056\n",
       " -0.767168  -0.732134  -0.809346     -0.84336   -0.834856  -0.816488"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xboth1=XNormalized\n",
    "Xboth2=XStandarized\n",
    "############### Normalize the standar version\n",
    "dt = fit(UnitRangeTransform, Xboth2, dims=2)\n",
    "StatsBase.transform(dt, Xboth2)\n",
    "############### Standarize the Normal version\n",
    "dt = fit(ZScoreTransform, Xboth1, dims=2)\n",
    "StatsBase.transform(dt, Xboth1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c43a989a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sonarlabelsmap=labelmap(sonarlabels); #The function  'labelmap'\n",
    "#find how manny labels and which are\n",
    "#y=labelencode(sonarlabelsmap, sonarlabels) # this function transform in anumerical label our stringslabels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295df64e",
   "metadata": {},
   "source": [
    " ### 4.  We want to use some of the data for training and the rest for testing, let's to create a funtion for the split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d2fbb09d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "perclass_splits (generic function with 1 method)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function perclass_splits(y,at) #arguments are the labels and the probability indepen for split\n",
    "    uids = unique(y) # this function return unique elements from a collection\n",
    "    keepids = []\n",
    "    #\n",
    "    for ui in uids\n",
    "        curids = findall(y.==ui)#find the coordinates where are equal the arrays\n",
    "        rowids = randsubseq(curids, at) #reorden for the coordinates \n",
    "        push!(keepids,rowids...) #savethem\n",
    "    end\n",
    "    return keepids\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "836b20b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140-element Vector{Any}:\n",
       "   2\n",
       "   5\n",
       "   6\n",
       "   7\n",
       "   8\n",
       "  11\n",
       "  12\n",
       "  14\n",
       "  15\n",
       "  16\n",
       "  17\n",
       "  18\n",
       "  19\n",
       "   ⋮\n",
       " 189\n",
       " 190\n",
       " 191\n",
       " 192\n",
       " 193\n",
       " 196\n",
       " 198\n",
       " 200\n",
       " 202\n",
       " 204\n",
       " 206\n",
       " 207"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainingdataindex=perclass_splits(y,0.67)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "2460cd45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40-element Vector{Int64}:\n",
       "  4\n",
       "  5\n",
       "  7\n",
       "  8\n",
       " 10\n",
       " 12\n",
       " 16\n",
       " 17\n",
       " 18\n",
       " 19\n",
       " 24\n",
       " 25\n",
       " 26\n",
       "  ⋮\n",
       " 49\n",
       " 50\n",
       " 52\n",
       " 53\n",
       " 55\n",
       " 56\n",
       " 58\n",
       " 60\n",
       " 61\n",
       " 63\n",
       " 66\n",
       " 67"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "restindex=setdiff(1:length(y), trainingdataindex)\n",
    "validationindex=perclass_splits(restindex,0.5)\n",
    "\n",
    "testdataindex=setdiff(1:length(restindex), validationindex)  \n",
    "#this function save the rest of a set in an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "beb21500",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27-element Vector{Any}:\n",
       "  1\n",
       "  2\n",
       "  3\n",
       "  6\n",
       "  9\n",
       " 11\n",
       " 13\n",
       " 14\n",
       " 15\n",
       " 20\n",
       " 21\n",
       " 22\n",
       " 23\n",
       "  ⋮\n",
       " 32\n",
       " 34\n",
       " 35\n",
       " 36\n",
       " 45\n",
       " 51\n",
       " 54\n",
       " 57\n",
       " 59\n",
       " 62\n",
       " 64\n",
       " 65"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validationindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f02c510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the best lambda to predict with.\n",
    "path = glmnet(X[trainids,:], y[trainids],alpha=0);\n",
    "cv = glmnetcv(X[trainids,:], y[trainids],alpha=0)\n",
    "mylambda = path.lambda[argmin(cv.meanloss)]\n",
    "path = glmnet(X[trainids,:], y[trainids],alpha=0,lambda=[mylambda]);\n",
    "q = X[testids,:];\n",
    "predictions_ridge = GLMNet.predict(path,q)\n",
    "predictions_ridge = assign_class.(predictions_ridge)\n",
    "findaccuracy(predictions_ridge,y[testids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "3e49d4f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1mg\u001b[22m\u001b[0m\u001b[1ml\u001b[22m\u001b[0m\u001b[1mm\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1mt\u001b[22m \u001b[0m\u001b[1mg\u001b[22m\u001b[0m\u001b[1ml\u001b[22m\u001b[0m\u001b[1mm\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1mt\u001b[22m! \u001b[0m\u001b[1mg\u001b[22m\u001b[0m\u001b[1ml\u001b[22m\u001b[0m\u001b[1mm\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1mt\u001b[22mcv \u001b[0m\u001b[1mG\u001b[22m\u001b[0m\u001b[1mL\u001b[22m\u001b[0m\u001b[1mM\u001b[22m\u001b[0m\u001b[1mN\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1mt\u001b[22m \u001b[0m\u001b[1mG\u001b[22m\u001b[0m\u001b[1mL\u001b[22m\u001b[0m\u001b[1mM\u001b[22m\u001b[0m\u001b[1mN\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1mt\u001b[22mPath \u001b[0m\u001b[1mG\u001b[22m\u001b[0m\u001b[1mL\u001b[22m\u001b[0m\u001b[1mM\u001b[22m\u001b[0m\u001b[1mN\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1mt\u001b[22mCrossValidation\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "No documentation found.\n",
       "\n",
       "\\texttt{GLMNet.glmnet} is a \\texttt{Function}.\n",
       "\n",
       "\\begin{verbatim}\n",
       "# 16 methods for generic function \"glmnet\":\n",
       "[1] glmnet(X::SparseArrays.SparseMatrixCSC, y::AbstractVector{<:Number}) in GLMNet at /Users/j1/.julia/packages/GLMNet/C8WKF/src/GLMNet.jl:489\n",
       "[2] glmnet(X::SparseArrays.SparseMatrixCSC, y::AbstractVector{<:Number}, family::Distributions.Distribution; kw...) in GLMNet at /Users/j1/.julia/packages/GLMNet/C8WKF/src/GLMNet.jl:489\n",
       "[3] glmnet(X::SparseArrays.SparseMatrixCSC, y::AbstractMatrix, family::Distributions.Binomial; kw...) in GLMNet at /Users/j1/.julia/packages/GLMNet/C8WKF/src/GLMNet.jl:493\n",
       "[4] glmnet(X::Matrix{Float64}, y::Matrix{Float64}, family::Distributions.Binomial; kw...) in GLMNet at /Users/j1/.julia/packages/GLMNet/C8WKF/src/GLMNet.jl:491\n",
       "[5] glmnet(X::Matrix, y::Matrix, family::Distributions.Binomial; kw...) in GLMNet at /Users/j1/.julia/packages/GLMNet/C8WKF/src/GLMNet.jl:495\n",
       "[6] glmnet(X::Matrix{Float64}, y::Matrix{Float64}, family::Distributions.Multinomial; kw...) in GLMNet at /Users/j1/.julia/packages/GLMNet/C8WKF/src/Multinomial.jl:185\n",
       "[7] glmnet(X::Matrix{Float64}, y::Vector{Float64}) in GLMNet at /Users/j1/.julia/packages/GLMNet/C8WKF/src/GLMNet.jl:485\n",
       "[8] glmnet(X::AbstractMatrix, y::AbstractMatrix, family::Distributions.Multinomial; kw...) in GLMNet at /Users/j1/.julia/packages/GLMNet/C8WKF/src/Multinomial.jl:188\n",
       "[9] glmnet(X::AbstractMatrix, time::AbstractVector, status::AbstractVector) in GLMNet at /Users/j1/.julia/packages/GLMNet/C8WKF/src/CoxNet.jl:151\n",
       "[10] glmnet(X::AbstractMatrix, time::AbstractVector, status::AbstractVector, family::CoxPH; kw...) in GLMNet at /Users/j1/.julia/packages/GLMNet/C8WKF/src/CoxNet.jl:151\n",
       "[11] glmnet(X::Matrix{Float64}, y::Vector{Float64}, family::Distributions.Distribution; kw...) in GLMNet at /Users/j1/.julia/packages/GLMNet/C8WKF/src/GLMNet.jl:485\n",
       "[12] glmnet(X::Matrix{Float64}, y::Matrix{Float64}, family::CoxPH; kw...) in GLMNet at /Users/j1/.julia/packages/GLMNet/C8WKF/src/CoxNet.jl:145\n",
       "[13] glmnet(X::AbstractMatrix, y::AbstractVector{<:Number}) in GLMNet at /Users/j1/.julia/packages/GLMNet/C8WKF/src/GLMNet.jl:487\n",
       "[14] glmnet(X::AbstractMatrix, y::AbstractVector{<:Number}, family::Distributions.Distribution; kw...) in GLMNet at /Users/j1/.julia/packages/GLMNet/C8WKF/src/GLMNet.jl:487\n",
       "[15] glmnet(X::AbstractMatrix, y::AbstractMatrix, family::CoxPH; kw...) in GLMNet at /Users/j1/.julia/packages/GLMNet/C8WKF/src/CoxNet.jl:148\n",
       "[16] glmnet(X::AbstractMatrix, y; kw...) in GLMNet at /Users/j1/.julia/packages/GLMNet/C8WKF/src/Multinomial.jl:191\n",
       "\\end{verbatim}\n"
      ],
      "text/markdown": [
       "No documentation found.\n",
       "\n",
       "`GLMNet.glmnet` is a `Function`.\n",
       "\n",
       "```\n",
       "# 16 methods for generic function \"glmnet\":\n",
       "[1] glmnet(X::SparseArrays.SparseMatrixCSC, y::AbstractVector{<:Number}) in GLMNet at /Users/j1/.julia/packages/GLMNet/C8WKF/src/GLMNet.jl:489\n",
       "[2] glmnet(X::SparseArrays.SparseMatrixCSC, y::AbstractVector{<:Number}, family::Distributions.Distribution; kw...) in GLMNet at /Users/j1/.julia/packages/GLMNet/C8WKF/src/GLMNet.jl:489\n",
       "[3] glmnet(X::SparseArrays.SparseMatrixCSC, y::AbstractMatrix, family::Distributions.Binomial; kw...) in GLMNet at /Users/j1/.julia/packages/GLMNet/C8WKF/src/GLMNet.jl:493\n",
       "[4] glmnet(X::Matrix{Float64}, y::Matrix{Float64}, family::Distributions.Binomial; kw...) in GLMNet at /Users/j1/.julia/packages/GLMNet/C8WKF/src/GLMNet.jl:491\n",
       "[5] glmnet(X::Matrix, y::Matrix, family::Distributions.Binomial; kw...) in GLMNet at /Users/j1/.julia/packages/GLMNet/C8WKF/src/GLMNet.jl:495\n",
       "[6] glmnet(X::Matrix{Float64}, y::Matrix{Float64}, family::Distributions.Multinomial; kw...) in GLMNet at /Users/j1/.julia/packages/GLMNet/C8WKF/src/Multinomial.jl:185\n",
       "[7] glmnet(X::Matrix{Float64}, y::Vector{Float64}) in GLMNet at /Users/j1/.julia/packages/GLMNet/C8WKF/src/GLMNet.jl:485\n",
       "[8] glmnet(X::AbstractMatrix, y::AbstractMatrix, family::Distributions.Multinomial; kw...) in GLMNet at /Users/j1/.julia/packages/GLMNet/C8WKF/src/Multinomial.jl:188\n",
       "[9] glmnet(X::AbstractMatrix, time::AbstractVector, status::AbstractVector) in GLMNet at /Users/j1/.julia/packages/GLMNet/C8WKF/src/CoxNet.jl:151\n",
       "[10] glmnet(X::AbstractMatrix, time::AbstractVector, status::AbstractVector, family::CoxPH; kw...) in GLMNet at /Users/j1/.julia/packages/GLMNet/C8WKF/src/CoxNet.jl:151\n",
       "[11] glmnet(X::Matrix{Float64}, y::Vector{Float64}, family::Distributions.Distribution; kw...) in GLMNet at /Users/j1/.julia/packages/GLMNet/C8WKF/src/GLMNet.jl:485\n",
       "[12] glmnet(X::Matrix{Float64}, y::Matrix{Float64}, family::CoxPH; kw...) in GLMNet at /Users/j1/.julia/packages/GLMNet/C8WKF/src/CoxNet.jl:145\n",
       "[13] glmnet(X::AbstractMatrix, y::AbstractVector{<:Number}) in GLMNet at /Users/j1/.julia/packages/GLMNet/C8WKF/src/GLMNet.jl:487\n",
       "[14] glmnet(X::AbstractMatrix, y::AbstractVector{<:Number}, family::Distributions.Distribution; kw...) in GLMNet at /Users/j1/.julia/packages/GLMNet/C8WKF/src/GLMNet.jl:487\n",
       "[15] glmnet(X::AbstractMatrix, y::AbstractMatrix, family::CoxPH; kw...) in GLMNet at /Users/j1/.julia/packages/GLMNet/C8WKF/src/CoxNet.jl:148\n",
       "[16] glmnet(X::AbstractMatrix, y; kw...) in GLMNet at /Users/j1/.julia/packages/GLMNet/C8WKF/src/Multinomial.jl:191\n",
       "```\n"
      ],
      "text/plain": [
       "  No documentation found.\n",
       "\n",
       "  \u001b[36mGLMNet.glmnet\u001b[39m is a \u001b[36mFunction\u001b[39m.\n",
       "\n",
       "\u001b[36m  # 16 methods for generic function \"glmnet\":\u001b[39m\n",
       "\u001b[36m  [1] glmnet(X::SparseArrays.SparseMatrixCSC, y::AbstractVector{<:Number}) in GLMNet at /Users/j1/.julia/packages/GLMNet/C8WKF/src/GLMNet.jl:489\u001b[39m\n",
       "\u001b[36m  [2] glmnet(X::SparseArrays.SparseMatrixCSC, y::AbstractVector{<:Number}, family::Distributions.Distribution; kw...) in GLMNet at /Users/j1/.julia/packages/GLMNet/C8WKF/src/GLMNet.jl:489\u001b[39m\n",
       "\u001b[36m  [3] glmnet(X::SparseArrays.SparseMatrixCSC, y::AbstractMatrix, family::Distributions.Binomial; kw...) in GLMNet at /Users/j1/.julia/packages/GLMNet/C8WKF/src/GLMNet.jl:493\u001b[39m\n",
       "\u001b[36m  [4] glmnet(X::Matrix{Float64}, y::Matrix{Float64}, family::Distributions.Binomial; kw...) in GLMNet at /Users/j1/.julia/packages/GLMNet/C8WKF/src/GLMNet.jl:491\u001b[39m\n",
       "\u001b[36m  [5] glmnet(X::Matrix, y::Matrix, family::Distributions.Binomial; kw...) in GLMNet at /Users/j1/.julia/packages/GLMNet/C8WKF/src/GLMNet.jl:495\u001b[39m\n",
       "\u001b[36m  [6] glmnet(X::Matrix{Float64}, y::Matrix{Float64}, family::Distributions.Multinomial; kw...) in GLMNet at /Users/j1/.julia/packages/GLMNet/C8WKF/src/Multinomial.jl:185\u001b[39m\n",
       "\u001b[36m  [7] glmnet(X::Matrix{Float64}, y::Vector{Float64}) in GLMNet at /Users/j1/.julia/packages/GLMNet/C8WKF/src/GLMNet.jl:485\u001b[39m\n",
       "\u001b[36m  [8] glmnet(X::AbstractMatrix, y::AbstractMatrix, family::Distributions.Multinomial; kw...) in GLMNet at /Users/j1/.julia/packages/GLMNet/C8WKF/src/Multinomial.jl:188\u001b[39m\n",
       "\u001b[36m  [9] glmnet(X::AbstractMatrix, time::AbstractVector, status::AbstractVector) in GLMNet at /Users/j1/.julia/packages/GLMNet/C8WKF/src/CoxNet.jl:151\u001b[39m\n",
       "\u001b[36m  [10] glmnet(X::AbstractMatrix, time::AbstractVector, status::AbstractVector, family::CoxPH; kw...) in GLMNet at /Users/j1/.julia/packages/GLMNet/C8WKF/src/CoxNet.jl:151\u001b[39m\n",
       "\u001b[36m  [11] glmnet(X::Matrix{Float64}, y::Vector{Float64}, family::Distributions.Distribution; kw...) in GLMNet at /Users/j1/.julia/packages/GLMNet/C8WKF/src/GLMNet.jl:485\u001b[39m\n",
       "\u001b[36m  [12] glmnet(X::Matrix{Float64}, y::Matrix{Float64}, family::CoxPH; kw...) in GLMNet at /Users/j1/.julia/packages/GLMNet/C8WKF/src/CoxNet.jl:145\u001b[39m\n",
       "\u001b[36m  [13] glmnet(X::AbstractMatrix, y::AbstractVector{<:Number}) in GLMNet at /Users/j1/.julia/packages/GLMNet/C8WKF/src/GLMNet.jl:487\u001b[39m\n",
       "\u001b[36m  [14] glmnet(X::AbstractMatrix, y::AbstractVector{<:Number}, family::Distributions.Distribution; kw...) in GLMNet at /Users/j1/.julia/packages/GLMNet/C8WKF/src/GLMNet.jl:487\u001b[39m\n",
       "\u001b[36m  [15] glmnet(X::AbstractMatrix, y::AbstractMatrix, family::CoxPH; kw...) in GLMNet at /Users/j1/.julia/packages/GLMNet/C8WKF/src/CoxNet.jl:148\u001b[39m\n",
       "\u001b[36m  [16] glmnet(X::AbstractMatrix, y; kw...) in GLMNet at /Users/j1/.julia/packages/GLMNet/C8WKF/src/Multinomial.jl:191\u001b[39m"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?glmnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c174b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "625280f4",
   "metadata": {},
   "source": [
    "## Choosing the best accuracy measures for this problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3287a13f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "assign_class (generic function with 1 method)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This function is a simple way to assing class to our predicted values\n",
    "assign_class(predictedvalue) = argmin(abs.(predictedvalue .- [1,2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c76197c",
   "metadata": {},
   "source": [
    "### About the order : \n",
    "#### 1. Training the Machines with the fives mods on the data set. (No feature Engineering, Standarized, Normalized, Both1 and Both2)\n",
    "#### 2. Validation for each machine (Looking for the best Hyperparameters)\n",
    "#### 3. Testing the models\n",
    "#### 4. Conclutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f26e5b",
   "metadata": {},
   "source": [
    "\n",
    "## Support Vector Machine\n",
    "\n",
    "We'll to use 'svmtrain' funtion from LIBSVM library to train our algorithm, and 'svmpredict' to predict in the testing  procces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a61fa7",
   "metadata": {},
   "source": [
    "### No feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "7382d7a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27-element PooledArrays.PooledVector{String1, UInt32, Vector{UInt32}}:\n",
       " \"R\"\n",
       " \"R\"\n",
       " \"R\"\n",
       " \"R\"\n",
       " \"R\"\n",
       " \"R\"\n",
       " \"R\"\n",
       " \"R\"\n",
       " \"R\"\n",
       " \"R\"\n",
       " \"R\"\n",
       " \"R\"\n",
       " \"R\"\n",
       " ⋮\n",
       " \"R\"\n",
       " \"R\"\n",
       " \"R\"\n",
       " \"R\"\n",
       " \"R\"\n",
       " \"R\"\n",
       " \"R\"\n",
       " \"R\"\n",
       " \"R\"\n",
       " \"R\"\n",
       " \"R\"\n",
       " \"R\""
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtraining=X[trainingdataindex,:] #Now, we'ill save the respective index data for the model\n",
    "Xtesting=X[testdataindex,:]\n",
    "Xvalidation=X[validationindex,:]\n",
    "ytraining=y[trainingdataindex]\n",
    "ytesting=y[testdataindex];\n",
    "yvalidation=y[validationindex];\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b77cf40e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mv\u001b[22m\u001b[0m\u001b[1mm\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1mr\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1mi\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "svmtrain(\n",
       "    X::AbstractMatrix{U}, y::AbstractVector{T} = [];\n",
       "    svmtype::Type = SVC,\n",
       "    kernel = Kernel.RadialBasis,\n",
       "    degree::Integer = 3,\n",
       "    gamma::Float64 = 1.0/size(X, 1),\n",
       "    coef0::Float64 = 0.0,\n",
       "    cost::Float64=1.0,\n",
       "    nu::Float64 = 0.5,\n",
       "    epsilon::Float64 = 0.1,\n",
       "    tolerance::Float64 = 0.001,\n",
       "    shrinking::Bool = true,\n",
       "    probability::Bool = false,\n",
       "    weights::Union{Dict{T,Float64},Cvoid} = nothing,\n",
       "    cachesize::Float64 = 200.0,\n",
       "    verbose::Bool = false\n",
       ") where {T,U<:Real}\n",
       "\\end{verbatim}\n",
       "Train Support Vector Machine using LIBSVM using response vector \\texttt{y} and training data \\texttt{X}. The shape of \\texttt{X} needs to be \\texttt{(nfeatures, nsamples)} or \\texttt{(nsamples, nsamples)} in the case of precomputed kernel (see below). For one-class SVM use only \\texttt{X}.\n",
       "\n",
       "\\section{Arguments}\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{svmtype::Type = LIBSVM.SVC}: Type of SVM to train \\texttt{SVC} (for C-SVM), \\texttt{NuSVC}   \\texttt{OneClassSVM}, \\texttt{EpsilonSVR} or \\texttt{NuSVR}. Defaults to \\texttt{OneClassSVM} if   \\texttt{y} is not used.\n",
       "\n",
       "\n",
       "\\item \\texttt{kernel = Kernel.RadialBasis}: Model kernel \\texttt{Kernels.Linear}, \\texttt{Kernels.Polynomial},   \\texttt{Kernels.RadialBasis}, \\texttt{Kernels.Sigmoid}, \\texttt{Kernels.Precomputed} or a \\texttt{Base.Callable}.\n",
       "\n",
       "\n",
       "\\item \\texttt{degree::Integer = 3}: Kernel degree. Used for polynomial kernel\n",
       "\n",
       "\n",
       "\\item \\texttt{gamma::Float64 = 1.0/size(X, 1)} : γ for kernels\n",
       "\n",
       "\n",
       "\\item \\texttt{coef0::Float64 = 0.0}: parameter for sigmoid and polynomial kernel\n",
       "\n",
       "\n",
       "\\item \\texttt{cost::Float64 = 1.0}: cost parameter C of C-SVC, epsilon-SVR, and nu-SVR\n",
       "\n",
       "\n",
       "\\item \\texttt{nu::Float64 = 0.5}: parameter nu of nu-SVC, one-class SVM, and nu-SVR\n",
       "\n",
       "\n",
       "\\item \\texttt{epsilon::Float64 = 0.1}: epsilon in loss function of epsilon-SVR\n",
       "\n",
       "\n",
       "\\item \\texttt{tolerance::Float64 = 0.001}: tolerance of termination criterion\n",
       "\n",
       "\n",
       "\\item \\texttt{shrinking::Bool = true}: whether to use the shrinking heuristics\n",
       "\n",
       "\n",
       "\\item \\texttt{probability::Bool = false}: whether to train a SVC or SVR model for probability estimates\n",
       "\n",
       "\n",
       "\\item \\texttt{weights::Union\\{Dict\\{T, Float64\\}, Cvoid\\} = nothing}: dictionary of class weights\n",
       "\n",
       "\n",
       "\\item \\texttt{cachesize::Float64 = 200.0}: cache memory size in MB\n",
       "\n",
       "\n",
       "\\item \\texttt{verbose::Bool = false}: print training output from LIBSVM if true\n",
       "\n",
       "\n",
       "\\item \\texttt{nt::Integer = 0}: number of OpenMP cores to use, if 0 it is set to OMP\\emph{NUM}THREADS, if negative it is set to the max number of threads\n",
       "\n",
       "\\end{itemize}\n",
       "Consult LIBSVM documentation for advice on the choise of correct parameters and model tuning.\n",
       "\n",
       "\\section{Precomputed kernel}\n",
       "In the case of precomputed kernel, the input matrix \\texttt{X} should be a (symmetric) matrix of shape \\texttt{(n, n)} where column \\texttt{i} contains values \\texttt{[K(x\\_i, x\\_1), K(x\\_i, x\\_2), ..., K(x\\_i, x\\_n)]}. For example, if matrix \\texttt{M} contains instances in its columns, then \\texttt{M' * M} produces the correct input matrix \\texttt{X} for linear kernel.\n",
       "\n"
      ],
      "text/markdown": [
       "```\n",
       "svmtrain(\n",
       "    X::AbstractMatrix{U}, y::AbstractVector{T} = [];\n",
       "    svmtype::Type = SVC,\n",
       "    kernel = Kernel.RadialBasis,\n",
       "    degree::Integer = 3,\n",
       "    gamma::Float64 = 1.0/size(X, 1),\n",
       "    coef0::Float64 = 0.0,\n",
       "    cost::Float64=1.0,\n",
       "    nu::Float64 = 0.5,\n",
       "    epsilon::Float64 = 0.1,\n",
       "    tolerance::Float64 = 0.001,\n",
       "    shrinking::Bool = true,\n",
       "    probability::Bool = false,\n",
       "    weights::Union{Dict{T,Float64},Cvoid} = nothing,\n",
       "    cachesize::Float64 = 200.0,\n",
       "    verbose::Bool = false\n",
       ") where {T,U<:Real}\n",
       "```\n",
       "\n",
       "Train Support Vector Machine using LIBSVM using response vector `y` and training data `X`. The shape of `X` needs to be `(nfeatures, nsamples)` or `(nsamples, nsamples)` in the case of precomputed kernel (see below). For one-class SVM use only `X`.\n",
       "\n",
       "# Arguments\n",
       "\n",
       "  * `svmtype::Type = LIBSVM.SVC`: Type of SVM to train `SVC` (for C-SVM), `NuSVC`   `OneClassSVM`, `EpsilonSVR` or `NuSVR`. Defaults to `OneClassSVM` if   `y` is not used.\n",
       "  * `kernel = Kernel.RadialBasis`: Model kernel `Kernels.Linear`, `Kernels.Polynomial`,   `Kernels.RadialBasis`, `Kernels.Sigmoid`, `Kernels.Precomputed` or a `Base.Callable`.\n",
       "  * `degree::Integer = 3`: Kernel degree. Used for polynomial kernel\n",
       "  * `gamma::Float64 = 1.0/size(X, 1)` : γ for kernels\n",
       "  * `coef0::Float64 = 0.0`: parameter for sigmoid and polynomial kernel\n",
       "  * `cost::Float64 = 1.0`: cost parameter C of C-SVC, epsilon-SVR, and nu-SVR\n",
       "  * `nu::Float64 = 0.5`: parameter nu of nu-SVC, one-class SVM, and nu-SVR\n",
       "  * `epsilon::Float64 = 0.1`: epsilon in loss function of epsilon-SVR\n",
       "  * `tolerance::Float64 = 0.001`: tolerance of termination criterion\n",
       "  * `shrinking::Bool = true`: whether to use the shrinking heuristics\n",
       "  * `probability::Bool = false`: whether to train a SVC or SVR model for probability estimates\n",
       "  * `weights::Union{Dict{T, Float64}, Cvoid} = nothing`: dictionary of class weights\n",
       "  * `cachesize::Float64 = 200.0`: cache memory size in MB\n",
       "  * `verbose::Bool = false`: print training output from LIBSVM if true\n",
       "  * `nt::Integer = 0`: number of OpenMP cores to use, if 0 it is set to OMP*NUM*THREADS, if negative it is set to the max number of threads\n",
       "\n",
       "Consult LIBSVM documentation for advice on the choise of correct parameters and model tuning.\n",
       "\n",
       "# Precomputed kernel\n",
       "\n",
       "In the case of precomputed kernel, the input matrix `X` should be a (symmetric) matrix of shape `(n, n)` where column `i` contains values `[K(x_i, x_1), K(x_i, x_2), ..., K(x_i, x_n)]`. For example, if matrix `M` contains instances in its columns, then `M' * M` produces the correct input matrix `X` for linear kernel.\n"
      ],
      "text/plain": [
       "\u001b[36m  svmtrain(\u001b[39m\n",
       "\u001b[36m      X::AbstractMatrix{U}, y::AbstractVector{T} = [];\u001b[39m\n",
       "\u001b[36m      svmtype::Type = SVC,\u001b[39m\n",
       "\u001b[36m      kernel = Kernel.RadialBasis,\u001b[39m\n",
       "\u001b[36m      degree::Integer = 3,\u001b[39m\n",
       "\u001b[36m      gamma::Float64 = 1.0/size(X, 1),\u001b[39m\n",
       "\u001b[36m      coef0::Float64 = 0.0,\u001b[39m\n",
       "\u001b[36m      cost::Float64=1.0,\u001b[39m\n",
       "\u001b[36m      nu::Float64 = 0.5,\u001b[39m\n",
       "\u001b[36m      epsilon::Float64 = 0.1,\u001b[39m\n",
       "\u001b[36m      tolerance::Float64 = 0.001,\u001b[39m\n",
       "\u001b[36m      shrinking::Bool = true,\u001b[39m\n",
       "\u001b[36m      probability::Bool = false,\u001b[39m\n",
       "\u001b[36m      weights::Union{Dict{T,Float64},Cvoid} = nothing,\u001b[39m\n",
       "\u001b[36m      cachesize::Float64 = 200.0,\u001b[39m\n",
       "\u001b[36m      verbose::Bool = false\u001b[39m\n",
       "\u001b[36m  ) where {T,U<:Real}\u001b[39m\n",
       "\n",
       "  Train Support Vector Machine using LIBSVM using response vector \u001b[36my\u001b[39m and\n",
       "  training data \u001b[36mX\u001b[39m. The shape of \u001b[36mX\u001b[39m needs to be \u001b[36m(nfeatures, nsamples)\u001b[39m or\n",
       "  \u001b[36m(nsamples, nsamples)\u001b[39m in the case of precomputed kernel (see below). For\n",
       "  one-class SVM use only \u001b[36mX\u001b[39m.\n",
       "\n",
       "\u001b[1m  Arguments\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "    •  \u001b[36msvmtype::Type = LIBSVM.SVC\u001b[39m: Type of SVM to train \u001b[36mSVC\u001b[39m (for C-SVM),\n",
       "       \u001b[36mNuSVC\u001b[39m \u001b[36mOneClassSVM\u001b[39m, \u001b[36mEpsilonSVR\u001b[39m or \u001b[36mNuSVR\u001b[39m. Defaults to \u001b[36mOneClassSVM\u001b[39m if\n",
       "       \u001b[36my\u001b[39m is not used.\n",
       "\n",
       "    •  \u001b[36mkernel = Kernel.RadialBasis\u001b[39m: Model kernel \u001b[36mKernels.Linear\u001b[39m,\n",
       "       \u001b[36mKernels.Polynomial\u001b[39m, \u001b[36mKernels.RadialBasis\u001b[39m, \u001b[36mKernels.Sigmoid\u001b[39m,\n",
       "       \u001b[36mKernels.Precomputed\u001b[39m or a \u001b[36mBase.Callable\u001b[39m.\n",
       "\n",
       "    •  \u001b[36mdegree::Integer = 3\u001b[39m: Kernel degree. Used for polynomial kernel\n",
       "\n",
       "    •  \u001b[36mgamma::Float64 = 1.0/size(X, 1)\u001b[39m : γ for kernels\n",
       "\n",
       "    •  \u001b[36mcoef0::Float64 = 0.0\u001b[39m: parameter for sigmoid and polynomial kernel\n",
       "\n",
       "    •  \u001b[36mcost::Float64 = 1.0\u001b[39m: cost parameter C of C-SVC, epsilon-SVR, and\n",
       "       nu-SVR\n",
       "\n",
       "    •  \u001b[36mnu::Float64 = 0.5\u001b[39m: parameter nu of nu-SVC, one-class SVM, and\n",
       "       nu-SVR\n",
       "\n",
       "    •  \u001b[36mepsilon::Float64 = 0.1\u001b[39m: epsilon in loss function of epsilon-SVR\n",
       "\n",
       "    •  \u001b[36mtolerance::Float64 = 0.001\u001b[39m: tolerance of termination criterion\n",
       "\n",
       "    •  \u001b[36mshrinking::Bool = true\u001b[39m: whether to use the shrinking heuristics\n",
       "\n",
       "    •  \u001b[36mprobability::Bool = false\u001b[39m: whether to train a SVC or SVR model for\n",
       "       probability estimates\n",
       "\n",
       "    •  \u001b[36mweights::Union{Dict{T, Float64}, Cvoid} = nothing\u001b[39m: dictionary of\n",
       "       class weights\n",
       "\n",
       "    •  \u001b[36mcachesize::Float64 = 200.0\u001b[39m: cache memory size in MB\n",
       "\n",
       "    •  \u001b[36mverbose::Bool = false\u001b[39m: print training output from LIBSVM if true\n",
       "\n",
       "    •  \u001b[36mnt::Integer = 0\u001b[39m: number of OpenMP cores to use, if 0 it is set to\n",
       "       OMP\u001b[4mNUM\u001b[24mTHREADS, if negative it is set to the max number of threads\n",
       "\n",
       "  Consult LIBSVM documentation for advice on the choise of correct parameters\n",
       "  and model tuning.\n",
       "\n",
       "\u001b[1m  Precomputed kernel\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "  In the case of precomputed kernel, the input matrix \u001b[36mX\u001b[39m should be a\n",
       "  (symmetric) matrix of shape \u001b[36m(n, n)\u001b[39m where column \u001b[36mi\u001b[39m contains values \u001b[36m[K(x_i,\n",
       "  x_1), K(x_i, x_2), ..., K(x_i, x_n)]\u001b[39m. For example, if matrix \u001b[36mM\u001b[39m contains\n",
       "  instances in its columns, then \u001b[36mM' * M\u001b[39m produces the correct input matrix \u001b[36mX\u001b[39m\n",
       "  for linear kernel."
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?svmtrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "efe5cfb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LIBSVM.SVM{String1, LIBSVM.Kernel.KERNEL}(SVC, LIBSVM.Kernel.Linear, nothing, 60, 140, 2, String1[\"R\", \"M\"], Int32[1, 2], Float64[], Int32[], LIBSVM.SupportVectors{PooledArrays.PooledVector{String1, UInt32, Vector{UInt32}}, Matrix{Float64}}(92, Int32[47, 45], String1[\"R\", \"R\", \"R\", \"R\", \"R\", \"R\", \"R\", \"R\", \"R\", \"R\"  …  \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\"], [0.0262 0.0286 … 0.0303 0.026; 0.0582 0.0453 … 0.0353 0.0363; … ; 0.0095 0.0051 … 0.0036 0.0061; 0.0078 0.0062 … 0.0048 0.0115], Int32[1, 2, 3, 4, 5, 6, 7, 8, 10, 13  …  121, 122, 129, 130, 131, 132, 133, 138, 139, 140], LIBSVM.SVMNode[LIBSVM.SVMNode(1, 0.0262), LIBSVM.SVMNode(1, 0.0286), LIBSVM.SVMNode(1, 0.0317), LIBSVM.SVMNode(1, 0.0519), LIBSVM.SVMNode(1, 0.0223), LIBSVM.SVMNode(1, 0.0123), LIBSVM.SVMNode(1, 0.0079), LIBSVM.SVMNode(1, 0.0124), LIBSVM.SVMNode(1, 0.0352), LIBSVM.SVMNode(1, 0.0126)  …  LIBSVM.SVMNode(1, 0.0197), LIBSVM.SVMNode(1, 0.0394), LIBSVM.SVMNode(1, 0.0158), LIBSVM.SVMNode(1, 0.0156), LIBSVM.SVMNode(1, 0.0315), LIBSVM.SVMNode(1, 0.0056), LIBSVM.SVMNode(1, 0.0203), LIBSVM.SVMNode(1, 0.0323), LIBSVM.SVMNode(1, 0.0303), LIBSVM.SVMNode(1, 0.026)]), 0.0, [1.0; 0.8427271928592178; … ; -0.8440933178098078; -1.0;;], Float64[], Float64[], [-2.3039579274609387], 3, 0.016666666666666666, 200.0, 0.001, 1.0, 0.5, 0.1, true, false)"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelsvm=svmtrain(Xtraining',ytraining; kernel=Kernel.Linear) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdbd20c",
   "metadata": {},
   "source": [
    "Claim: When we change the Kernel for the model, in a 'svmtrain' function, the best accuracy with Linear Kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "00048aa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(String1[\"M\", \"R\", \"M\", \"M\", \"R\", \"R\", \"R\", \"M\", \"R\", \"R\"  …  \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\"], [-0.21640077559632065 0.9468880631396583 … -1.3570450990686815 -0.594821171279075; 0.0 0.0 … 0.0 0.0])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhatsvm, decision_values = svmpredict(modelsvm, Xtesting')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8e731a",
   "metadata": {},
   "source": [
    "### What's the best SVM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7d437523",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7164179104477612"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svmaccuraccy=findaccuracy(yhatsvm, ytesting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "406c69c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67×3 Matrix{Any}:\n",
       " \"M\"  \"R\"  false\n",
       " \"R\"  \"R\"   true\n",
       " \"M\"  \"R\"  false\n",
       " \"M\"  \"R\"  false\n",
       " \"R\"  \"R\"   true\n",
       " \"R\"  \"R\"   true\n",
       " \"R\"  \"R\"   true\n",
       " \"M\"  \"R\"  false\n",
       " \"R\"  \"R\"   true\n",
       " \"R\"  \"R\"   true\n",
       " \"R\"  \"R\"   true\n",
       " \"R\"  \"R\"   true\n",
       " \"R\"  \"R\"   true\n",
       " ⋮         \n",
       " \"M\"  \"M\"   true\n",
       " \"M\"  \"M\"   true\n",
       " \"M\"  \"M\"   true\n",
       " \"M\"  \"M\"   true\n",
       " \"M\"  \"M\"   true\n",
       " \"M\"  \"M\"   true\n",
       " \"M\"  \"M\"   true\n",
       " \"M\"  \"M\"   true\n",
       " \"M\"  \"M\"   true\n",
       " \"M\"  \"M\"   true\n",
       " \"M\"  \"M\"   true\n",
       " \"M\"  \"M\"   true"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###check the results\n",
    "checksvm=[yhatsvm[i] == ytesting[i] for i in 1:length(yhatsvm)]\n",
    "checksvmdisplay=[yhatsvm ytesting checksvm]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0951cf72",
   "metadata": {},
   "source": [
    "# Decision Tree\n",
    "Here, we'll to use the library DecisionTree, and its principal functions: 'DecisionTreeClassifier' and 'fit!' for to create a model bades on our data- finally, ' DecisionTree.predict' to testing our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "0304bb21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1mD\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1mc\u001b[22m\u001b[0m\u001b[1mi\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mi\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1mT\u001b[22m\u001b[0m\u001b[1mr\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1mC\u001b[22m\u001b[0m\u001b[1ml\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mi\u001b[22m\u001b[0m\u001b[1mf\u001b[22m\u001b[0m\u001b[1mi\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1mr\u001b[22m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "DecisionTreeClassifier(; pruning_purity_threshold=0.0,\n",
       "                       max_depth::Int=-1,\n",
       "                       min_samples_leaf::Int=1,\n",
       "                       min_samples_split::Int=2,\n",
       "                       min_purity_increase::Float=0.0,\n",
       "                       n_subfeatures::Int=0,\n",
       "                       rng=Random.GLOBAL_RNG)\n",
       "\\end{verbatim}\n",
       "Decision tree classifier. See \\href{https://github.com/bensadeghi/DecisionTree.jl}{DecisionTree.jl's documentation}\n",
       "\n",
       "Hyperparameters:\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{pruning\\_purity\\_threshold}: (post-pruning) merge leaves having \\texttt{>=thresh} combined purity (default: no pruning)\n",
       "\n",
       "\n",
       "\\item \\texttt{max\\_depth}: maximum depth of the decision tree (default: no maximum)\n",
       "\n",
       "\n",
       "\\item \\texttt{min\\_samples\\_leaf}: the minimum number of samples each leaf needs to have (default: 1)\n",
       "\n",
       "\n",
       "\\item \\texttt{min\\_samples\\_split}: the minimum number of samples in needed for a split (default: 2)\n",
       "\n",
       "\n",
       "\\item \\texttt{min\\_purity\\_increase}: minimum purity needed for a split (default: 0.0)\n",
       "\n",
       "\n",
       "\\item \\texttt{n\\_subfeatures}: number of features to select at random (default: keep all)\n",
       "\n",
       "\n",
       "\\item \\texttt{rng}: the random number generator to use. Can be an \\texttt{Int}, which will be used to seed and create a new random number generator.\n",
       "\n",
       "\\end{itemize}\n",
       "Implements \\texttt{fit!}, \\texttt{predict}, \\texttt{predict\\_proba}, \\texttt{get\\_classes}\n",
       "\n"
      ],
      "text/markdown": [
       "```\n",
       "DecisionTreeClassifier(; pruning_purity_threshold=0.0,\n",
       "                       max_depth::Int=-1,\n",
       "                       min_samples_leaf::Int=1,\n",
       "                       min_samples_split::Int=2,\n",
       "                       min_purity_increase::Float=0.0,\n",
       "                       n_subfeatures::Int=0,\n",
       "                       rng=Random.GLOBAL_RNG)\n",
       "```\n",
       "\n",
       "Decision tree classifier. See [DecisionTree.jl's documentation](https://github.com/bensadeghi/DecisionTree.jl)\n",
       "\n",
       "Hyperparameters:\n",
       "\n",
       "  * `pruning_purity_threshold`: (post-pruning) merge leaves having `>=thresh` combined purity (default: no pruning)\n",
       "  * `max_depth`: maximum depth of the decision tree (default: no maximum)\n",
       "  * `min_samples_leaf`: the minimum number of samples each leaf needs to have (default: 1)\n",
       "  * `min_samples_split`: the minimum number of samples in needed for a split (default: 2)\n",
       "  * `min_purity_increase`: minimum purity needed for a split (default: 0.0)\n",
       "  * `n_subfeatures`: number of features to select at random (default: keep all)\n",
       "  * `rng`: the random number generator to use. Can be an `Int`, which will be used to seed and create a new random number generator.\n",
       "\n",
       "Implements `fit!`, `predict`, `predict_proba`, `get_classes`\n"
      ],
      "text/plain": [
       "\u001b[36m  DecisionTreeClassifier(; pruning_purity_threshold=0.0,\u001b[39m\n",
       "\u001b[36m                         max_depth::Int=-1,\u001b[39m\n",
       "\u001b[36m                         min_samples_leaf::Int=1,\u001b[39m\n",
       "\u001b[36m                         min_samples_split::Int=2,\u001b[39m\n",
       "\u001b[36m                         min_purity_increase::Float=0.0,\u001b[39m\n",
       "\u001b[36m                         n_subfeatures::Int=0,\u001b[39m\n",
       "\u001b[36m                         rng=Random.GLOBAL_RNG)\u001b[39m\n",
       "\n",
       "  Decision tree classifier. See DecisionTree.jl's documentation\n",
       "  (https://github.com/bensadeghi/DecisionTree.jl)\n",
       "\n",
       "  Hyperparameters:\n",
       "\n",
       "    •  \u001b[36mpruning_purity_threshold\u001b[39m: (post-pruning) merge leaves having\n",
       "       \u001b[36m>=thresh\u001b[39m combined purity (default: no pruning)\n",
       "\n",
       "    •  \u001b[36mmax_depth\u001b[39m: maximum depth of the decision tree (default: no\n",
       "       maximum)\n",
       "\n",
       "    •  \u001b[36mmin_samples_leaf\u001b[39m: the minimum number of samples each leaf needs to\n",
       "       have (default: 1)\n",
       "\n",
       "    •  \u001b[36mmin_samples_split\u001b[39m: the minimum number of samples in needed for a\n",
       "       split (default: 2)\n",
       "\n",
       "    •  \u001b[36mmin_purity_increase\u001b[39m: minimum purity needed for a split (default:\n",
       "       0.0)\n",
       "\n",
       "    •  \u001b[36mn_subfeatures\u001b[39m: number of features to select at random (default:\n",
       "       keep all)\n",
       "\n",
       "    •  \u001b[36mrng\u001b[39m: the random number generator to use. Can be an \u001b[36mInt\u001b[39m, which will\n",
       "       be used to seed and create a new random number generator.\n",
       "\n",
       "  Implements \u001b[36mfit!\u001b[39m, \u001b[36mpredict\u001b[39m, \u001b[36mpredict_proba\u001b[39m, \u001b[36mget_classes\u001b[39m"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "50772e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1mD\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1mc\u001b[22m\u001b[0m\u001b[1mi\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mi\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1mT\u001b[22m\u001b[0m\u001b[1mr\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1me\u001b[22m \u001b[0m\u001b[1mD\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1mc\u001b[22m\u001b[0m\u001b[1mi\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mi\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1mT\u001b[22m\u001b[0m\u001b[1mr\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1me\u001b[22mRegressor \u001b[0m\u001b[1mD\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1mc\u001b[22m\u001b[0m\u001b[1mi\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mi\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1mT\u001b[22m\u001b[0m\u001b[1mr\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1me\u001b[22mClassifier\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "No docstring found for module \\texttt{DecisionTree}.\n",
       "\n",
       "\\section{Exported names}\n",
       "\\texttt{AdaBoostStumpClassifier}, \\texttt{ConfusionMatrix}, \\texttt{DecisionTreeClassifier}, \\texttt{DecisionTreeRegressor}, \\texttt{Ensemble}, \\texttt{Leaf}, \\texttt{Node}, \\texttt{R2}, \\texttt{RandomForestClassifier}, \\texttt{RandomForestRegressor}, \\texttt{apply\\_adaboost\\_stumps}, \\texttt{apply\\_adaboost\\_stumps\\_proba}, \\texttt{apply\\_forest}, \\texttt{apply\\_forest\\_proba}, \\texttt{apply\\_tree}, \\texttt{apply\\_tree\\_proba}, \\texttt{build\\_adaboost\\_stumps}, \\texttt{build\\_forest}, \\texttt{build\\_stump}, \\texttt{build\\_tree}, \\texttt{confusion\\_matrix}, \\texttt{depth}, \\texttt{fit!}, \\texttt{get\\_classes}, \\texttt{load\\_data}, \\texttt{majority\\_vote}, \\texttt{mean\\_squared\\_error}, \\texttt{nfoldCV\\_forest}, \\texttt{nfoldCV\\_stumps}, \\texttt{nfoldCV\\_tree}, \\texttt{predict}, \\texttt{predict\\_proba}, \\texttt{print\\_tree}, \\texttt{prune\\_tree}\n",
       "\n",
       "\\section{Displaying contents of readme found at \\texttt{/Users/j1/.julia/packages/DecisionTree/iWCbW/README.md}}\n",
       "\\section{DecisionTree.jl}\n",
       "\\href{https://github.com/bensadeghi/DecisionTree.jl/actions?query=workflow%3ACI}{\\begin{figure}\n",
       "\\centering\n",
       "\\includegraphics{https://github.com/bensadeghi/DecisionTree.jl/workflows/CI/badge.svg}\n",
       "\\caption{CI}\n",
       "\\end{figure}\n",
       "} \\href{https://codecov.io/gh/bensadeghi/DecisionTree.jl}{\\begin{figure}\n",
       "\\centering\n",
       "\\includegraphics{https://codecov.io/gh/bensadeghi/DecisionTree.jl/branch/master/graph/badge.svg}\n",
       "\\caption{Codecov}\n",
       "\\end{figure}\n",
       "} \\href{https://juliahub.com/docs/DecisionTree/pEDeB/0.10.11/}{\\begin{figure}\n",
       "\\centering\n",
       "\\includegraphics{https://img.shields.io/badge/docs-stable-blue.svg}\n",
       "\\caption{Docs Stable}\n",
       "\\end{figure}\n",
       "}\n",
       "\n",
       "Julia implementation of Decision Tree (CART) and Random Forest algorithms\n",
       "\n",
       "Available via:\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item \\href{https://github.com/IBM/AutoMLPipeline.jl}{AutoMLPipeline.jl} - create complex ML pipeline structures using simple expressions\n",
       "\n",
       "\n",
       "\\item \\href{https://github.com/ppalmes/CombineML.jl}{CombineML.jl} - a heterogeneous ensemble learning package\n",
       "\n",
       "\n",
       "\\item \\href{https://github.com/alan-turing-institute/MLJ.jl}{MLJ.jl} - a machine learning framework for Julia\n",
       "\n",
       "\n",
       "\\item \\href{https://github.com/cstjean/ScikitLearn.jl}{ScikitLearn.jl} - Julia implementation of the scikit-learn API\n",
       "\n",
       "\\end{itemize}\n",
       "\\subsection{Classification}\n",
       "\\begin{itemize}\n",
       "\\item pre-pruning (max depth, min leaf size)\n",
       "\n",
       "\n",
       "\\item post-pruning (pessimistic pruning)\n",
       "\n",
       "\n",
       "\\item multi-threaded bagging (random forests)\n",
       "\n",
       "\n",
       "\\item adaptive boosting (decision stumps)\n",
       "\n",
       "\n",
       "\\item cross validation (n-fold)\n",
       "\n",
       "\n",
       "\\item support for ordered features (encoded as \\texttt{Real}s or \\texttt{String}s)\n",
       "\n",
       "\\end{itemize}\n",
       "\\subsection{Regression}\n",
       "\\begin{itemize}\n",
       "\\item pre-pruning (max depth, min leaf size)\n",
       "\n",
       "\n",
       "\\item multi-threaded bagging (random forests)\n",
       "\n",
       "\n",
       "\\item cross validation (n-fold)\n",
       "\n",
       "\n",
       "\\item support for numerical features\n",
       "\n",
       "\\end{itemize}\n",
       "\\textbf{Note that regression is implied if labels/targets are of type Array\\{Float\\}}\n",
       "\n",
       "\\subsection{Installation}\n",
       "You can install DecisionTree.jl using Julia's package manager\n",
       "\n",
       "\\begin{verbatim}\n",
       "Pkg.add(\"DecisionTree\")\n",
       "\\end{verbatim}\n",
       "\\subsection{ScikitLearn.jl API}\n",
       "DecisionTree.jl supports the \\href{https://github.com/cstjean/ScikitLearn.jl}{ScikitLearn.jl} interface and algorithms (cross-validation, hyperparameter tuning, pipelines, etc.)\n",
       "\n",
       "Available models: \\texttt{DecisionTreeClassifier, DecisionTreeRegressor, RandomForestClassifier, RandomForestRegressor, AdaBoostStumpClassifier}. See each model's help (eg. \\texttt{?DecisionTreeRegressor} at the REPL) for more information\n",
       "\n",
       "\\subsubsection{Classification Example}\n",
       "Load DecisionTree package\n",
       "\n",
       "\\begin{verbatim}\n",
       "using DecisionTree\n",
       "\\end{verbatim}\n",
       "Separate Fisher's Iris dataset features and labels\n",
       "\n",
       "\\begin{verbatim}\n",
       "features, labels = load_data(\"iris\")    # also see \"adult\" and \"digits\" datasets\n",
       "\n",
       "# the data loaded are of type Array{Any}\n",
       "# cast them to concrete types for better performance\n",
       "features = float.(features)\n",
       "labels   = string.(labels)\n",
       "\\end{verbatim}\n",
       "Pruned Tree Classifier\n",
       "\n",
       "\\begin{verbatim}\n",
       "# train depth-truncated classifier\n",
       "model = DecisionTreeClassifier(max_depth=2)\n",
       "fit!(model, features, labels)\n",
       "# pretty print of the tree, to a depth of 5 nodes (optional)\n",
       "print_tree(model, 5)\n",
       "# apply learned model\n",
       "predict(model, [5.9,3.0,5.1,1.9])\n",
       "# get the probability of each label\n",
       "predict_proba(model, [5.9,3.0,5.1,1.9])\n",
       "println(get_classes(model)) # returns the ordering of the columns in predict_proba's output\n",
       "# run n-fold cross validation over 3 CV folds\n",
       "# See ScikitLearn.jl for installation instructions\n",
       "using ScikitLearn.CrossValidation: cross_val_score\n",
       "accuracy = cross_val_score(model, features, labels, cv=3)\n",
       "\\end{verbatim}\n",
       "Also, have a look at these \\href{https://github.com/cstjean/ScikitLearn.jl/blob/master/examples/Classifier_Comparison_Julia.ipynb}{classification} and \\href{https://github.com/cstjean/ScikitLearn.jl/blob/master/examples/Decision_Tree_Regression_Julia.ipynb}{regression} notebooks.\n",
       "\n",
       "\\subsection{Native API}\n",
       "\\subsubsection{Classification Example}\n",
       "Decision Tree Classifier\n",
       "\n",
       "\\begin{verbatim}\n",
       "# train full-tree classifier\n",
       "model = build_tree(labels, features)\n",
       "# prune tree: merge leaves having >= 90% combined purity (default: 100%)\n",
       "model = prune_tree(model, 0.9)\n",
       "# pretty print of the tree, to a depth of 5 nodes (optional)\n",
       "print_tree(model, 5)\n",
       "# apply learned model\n",
       "apply_tree(model, [5.9,3.0,5.1,1.9])\n",
       "# apply model to all the sames\n",
       "preds = apply_tree(model, features)\n",
       "# generate confusion matrix, along with accuracy and kappa scores\n",
       "confusion_matrix(labels, preds)\n",
       "# get the probability of each label\n",
       "apply_tree_proba(model, [5.9,3.0,5.1,1.9], [\"Iris-setosa\", \"Iris-versicolor\", \"Iris-virginica\"])\n",
       "# run 3-fold cross validation of pruned tree,\n",
       "n_folds=3\n",
       "accuracy = nfoldCV_tree(labels, features, n_folds)\n",
       "\n",
       "# set of classification parameters and respective default values\n",
       "# pruning_purity: purity threshold used for post-pruning (default: 1.0, no pruning)\n",
       "# max_depth: maximum depth of the decision tree (default: -1, no maximum)\n",
       "# min_samples_leaf: the minimum number of samples each leaf needs to have (default: 1)\n",
       "# min_samples_split: the minimum number of samples in needed for a split (default: 2)\n",
       "# min_purity_increase: minimum purity needed for a split (default: 0.0)\n",
       "# n_subfeatures: number of features to select at random (default: 0, keep all)\n",
       "# keyword rng: the random number generator or seed to use (default Random.GLOBAL_RNG)\n",
       "n_subfeatures=0; max_depth=-1; min_samples_leaf=1; min_samples_split=2\n",
       "min_purity_increase=0.0; pruning_purity = 1.0; seed=3\n",
       "\n",
       "model    =   build_tree(labels, features,\n",
       "                        n_subfeatures,\n",
       "                        max_depth,\n",
       "                        min_samples_leaf,\n",
       "                        min_samples_split,\n",
       "                        min_purity_increase;\n",
       "                        rng = seed)\n",
       "\n",
       "accuracy = nfoldCV_tree(labels, features,\n",
       "                        n_folds,\n",
       "                        pruning_purity,\n",
       "                        max_depth,\n",
       "                        min_samples_leaf,\n",
       "                        min_samples_split,\n",
       "                        min_purity_increase;\n",
       "                        verbose = true,\n",
       "                        rng = seed)\n",
       "\\end{verbatim}\n",
       "Random Forest Classifier\n",
       "\n",
       "\\begin{verbatim}\n",
       "# train random forest classifier\n",
       "# using 2 random features, 10 trees, 0.5 portion of samples per tree, and a maximum tree depth of 6\n",
       "model = build_forest(labels, features, 2, 10, 0.5, 6)\n",
       "# apply learned model\n",
       "apply_forest(model, [5.9,3.0,5.1,1.9])\n",
       "# get the probability of each label\n",
       "apply_forest_proba(model, [5.9,3.0,5.1,1.9], [\"Iris-setosa\", \"Iris-versicolor\", \"Iris-virginica\"])\n",
       "# run 3-fold cross validation for forests, using 2 random features per split\n",
       "n_folds=3; n_subfeatures=2\n",
       "accuracy = nfoldCV_forest(labels, features, n_folds, n_subfeatures)\n",
       "\n",
       "# set of classification parameters and respective default values\n",
       "# n_subfeatures: number of features to consider at random per split (default: -1, sqrt(# features))\n",
       "# n_trees: number of trees to train (default: 10)\n",
       "# partial_sampling: fraction of samples to train each tree on (default: 0.7)\n",
       "# max_depth: maximum depth of the decision trees (default: no maximum)\n",
       "# min_samples_leaf: the minimum number of samples each leaf needs to have (default: 5)\n",
       "# min_samples_split: the minimum number of samples in needed for a split (default: 2)\n",
       "# min_purity_increase: minimum purity needed for a split (default: 0.0)\n",
       "# keyword rng: the random number generator or seed to use (default Random.GLOBAL_RNG)\n",
       "#              multi-threaded forests must be seeded with an `Int`\n",
       "n_subfeatures=-1; n_trees=10; partial_sampling=0.7; max_depth=-1\n",
       "min_samples_leaf=5; min_samples_split=2; min_purity_increase=0.0; seed=3\n",
       "\n",
       "model    =   build_forest(labels, features,\n",
       "                          n_subfeatures,\n",
       "                          n_trees,\n",
       "                          partial_sampling,\n",
       "                          max_depth,\n",
       "                          min_samples_leaf,\n",
       "                          min_samples_split,\n",
       "                          min_purity_increase;\n",
       "                          rng = seed)\n",
       "\n",
       "accuracy = nfoldCV_forest(labels, features,\n",
       "                          n_folds,\n",
       "                          n_subfeatures,\n",
       "                          n_trees,\n",
       "                          partial_sampling,\n",
       "                          max_depth,\n",
       "                          min_samples_leaf,\n",
       "                          min_samples_split,\n",
       "                          min_purity_increase;\n",
       "                          verbose = true,\n",
       "                          rng = seed)\n",
       "\\end{verbatim}\n",
       "Adaptive-Boosted Decision Stumps Classifier\n",
       "\n",
       "\\begin{verbatim}\n",
       "# train adaptive-boosted stumps, using 7 iterations\n",
       "model, coeffs = build_adaboost_stumps(labels, features, 7);\n",
       "# apply learned model\n",
       "apply_adaboost_stumps(model, coeffs, [5.9,3.0,5.1,1.9])\n",
       "# get the probability of each label\n",
       "apply_adaboost_stumps_proba(model, coeffs, [5.9,3.0,5.1,1.9], [\"Iris-setosa\", \"Iris-versicolor\", \"Iris-virginica\"])\n",
       "# run 3-fold cross validation for boosted stumps, using 7 iterations\n",
       "n_iterations=7; n_folds=3\n",
       "accuracy = nfoldCV_stumps(labels, features,\n",
       "                          n_folds,\n",
       "                          n_iterations;\n",
       "                          verbose = true)\n",
       "\\end{verbatim}\n",
       "\\subsubsection{Regression Example}\n",
       "\\begin{verbatim}\n",
       "n, m = 10^3, 5\n",
       "features = randn(n, m)\n",
       "weights = rand(-2:2, m)\n",
       "labels = features * weights\n",
       "\\end{verbatim}\n",
       "Regression Tree ```julia\n",
       "\n",
       "[output truncated to first 200 lines]\n",
       "\n"
      ],
      "text/markdown": [
       "No docstring found for module `DecisionTree`.\n",
       "\n",
       "# Exported names\n",
       "\n",
       "`AdaBoostStumpClassifier`, `ConfusionMatrix`, `DecisionTreeClassifier`, `DecisionTreeRegressor`, `Ensemble`, `Leaf`, `Node`, `R2`, `RandomForestClassifier`, `RandomForestRegressor`, `apply_adaboost_stumps`, `apply_adaboost_stumps_proba`, `apply_forest`, `apply_forest_proba`, `apply_tree`, `apply_tree_proba`, `build_adaboost_stumps`, `build_forest`, `build_stump`, `build_tree`, `confusion_matrix`, `depth`, `fit!`, `get_classes`, `load_data`, `majority_vote`, `mean_squared_error`, `nfoldCV_forest`, `nfoldCV_stumps`, `nfoldCV_tree`, `predict`, `predict_proba`, `print_tree`, `prune_tree`\n",
       "\n",
       "# Displaying contents of readme found at `/Users/j1/.julia/packages/DecisionTree/iWCbW/README.md`\n",
       "\n",
       "# DecisionTree.jl\n",
       "\n",
       "[![CI](https://github.com/bensadeghi/DecisionTree.jl/workflows/CI/badge.svg)](https://github.com/bensadeghi/DecisionTree.jl/actions?query=workflow%3ACI) [![Codecov](https://codecov.io/gh/bensadeghi/DecisionTree.jl/branch/master/graph/badge.svg)](https://codecov.io/gh/bensadeghi/DecisionTree.jl) [![Docs Stable](https://img.shields.io/badge/docs-stable-blue.svg)](https://juliahub.com/docs/DecisionTree/pEDeB/0.10.11/)\n",
       "\n",
       "Julia implementation of Decision Tree (CART) and Random Forest algorithms\n",
       "\n",
       "Available via:\n",
       "\n",
       "  * [AutoMLPipeline.jl](https://github.com/IBM/AutoMLPipeline.jl) - create complex ML pipeline structures using simple expressions\n",
       "  * [CombineML.jl](https://github.com/ppalmes/CombineML.jl) - a heterogeneous ensemble learning package\n",
       "  * [MLJ.jl](https://github.com/alan-turing-institute/MLJ.jl) - a machine learning framework for Julia\n",
       "  * [ScikitLearn.jl](https://github.com/cstjean/ScikitLearn.jl) - Julia implementation of the scikit-learn API\n",
       "\n",
       "## Classification\n",
       "\n",
       "  * pre-pruning (max depth, min leaf size)\n",
       "  * post-pruning (pessimistic pruning)\n",
       "  * multi-threaded bagging (random forests)\n",
       "  * adaptive boosting (decision stumps)\n",
       "  * cross validation (n-fold)\n",
       "  * support for ordered features (encoded as `Real`s or `String`s)\n",
       "\n",
       "## Regression\n",
       "\n",
       "  * pre-pruning (max depth, min leaf size)\n",
       "  * multi-threaded bagging (random forests)\n",
       "  * cross validation (n-fold)\n",
       "  * support for numerical features\n",
       "\n",
       "**Note that regression is implied if labels/targets are of type Array{Float}**\n",
       "\n",
       "## Installation\n",
       "\n",
       "You can install DecisionTree.jl using Julia's package manager\n",
       "\n",
       "```julia\n",
       "Pkg.add(\"DecisionTree\")\n",
       "```\n",
       "\n",
       "## ScikitLearn.jl API\n",
       "\n",
       "DecisionTree.jl supports the [ScikitLearn.jl](https://github.com/cstjean/ScikitLearn.jl) interface and algorithms (cross-validation, hyperparameter tuning, pipelines, etc.)\n",
       "\n",
       "Available models: `DecisionTreeClassifier, DecisionTreeRegressor, RandomForestClassifier, RandomForestRegressor, AdaBoostStumpClassifier`. See each model's help (eg. `?DecisionTreeRegressor` at the REPL) for more information\n",
       "\n",
       "### Classification Example\n",
       "\n",
       "Load DecisionTree package\n",
       "\n",
       "```julia\n",
       "using DecisionTree\n",
       "```\n",
       "\n",
       "Separate Fisher's Iris dataset features and labels\n",
       "\n",
       "```julia\n",
       "features, labels = load_data(\"iris\")    # also see \"adult\" and \"digits\" datasets\n",
       "\n",
       "# the data loaded are of type Array{Any}\n",
       "# cast them to concrete types for better performance\n",
       "features = float.(features)\n",
       "labels   = string.(labels)\n",
       "```\n",
       "\n",
       "Pruned Tree Classifier\n",
       "\n",
       "```julia\n",
       "# train depth-truncated classifier\n",
       "model = DecisionTreeClassifier(max_depth=2)\n",
       "fit!(model, features, labels)\n",
       "# pretty print of the tree, to a depth of 5 nodes (optional)\n",
       "print_tree(model, 5)\n",
       "# apply learned model\n",
       "predict(model, [5.9,3.0,5.1,1.9])\n",
       "# get the probability of each label\n",
       "predict_proba(model, [5.9,3.0,5.1,1.9])\n",
       "println(get_classes(model)) # returns the ordering of the columns in predict_proba's output\n",
       "# run n-fold cross validation over 3 CV folds\n",
       "# See ScikitLearn.jl for installation instructions\n",
       "using ScikitLearn.CrossValidation: cross_val_score\n",
       "accuracy = cross_val_score(model, features, labels, cv=3)\n",
       "```\n",
       "\n",
       "Also, have a look at these [classification](https://github.com/cstjean/ScikitLearn.jl/blob/master/examples/Classifier_Comparison_Julia.ipynb) and [regression](https://github.com/cstjean/ScikitLearn.jl/blob/master/examples/Decision_Tree_Regression_Julia.ipynb) notebooks.\n",
       "\n",
       "## Native API\n",
       "\n",
       "### Classification Example\n",
       "\n",
       "Decision Tree Classifier\n",
       "\n",
       "```julia\n",
       "# train full-tree classifier\n",
       "model = build_tree(labels, features)\n",
       "# prune tree: merge leaves having >= 90% combined purity (default: 100%)\n",
       "model = prune_tree(model, 0.9)\n",
       "# pretty print of the tree, to a depth of 5 nodes (optional)\n",
       "print_tree(model, 5)\n",
       "# apply learned model\n",
       "apply_tree(model, [5.9,3.0,5.1,1.9])\n",
       "# apply model to all the sames\n",
       "preds = apply_tree(model, features)\n",
       "# generate confusion matrix, along with accuracy and kappa scores\n",
       "confusion_matrix(labels, preds)\n",
       "# get the probability of each label\n",
       "apply_tree_proba(model, [5.9,3.0,5.1,1.9], [\"Iris-setosa\", \"Iris-versicolor\", \"Iris-virginica\"])\n",
       "# run 3-fold cross validation of pruned tree,\n",
       "n_folds=3\n",
       "accuracy = nfoldCV_tree(labels, features, n_folds)\n",
       "\n",
       "# set of classification parameters and respective default values\n",
       "# pruning_purity: purity threshold used for post-pruning (default: 1.0, no pruning)\n",
       "# max_depth: maximum depth of the decision tree (default: -1, no maximum)\n",
       "# min_samples_leaf: the minimum number of samples each leaf needs to have (default: 1)\n",
       "# min_samples_split: the minimum number of samples in needed for a split (default: 2)\n",
       "# min_purity_increase: minimum purity needed for a split (default: 0.0)\n",
       "# n_subfeatures: number of features to select at random (default: 0, keep all)\n",
       "# keyword rng: the random number generator or seed to use (default Random.GLOBAL_RNG)\n",
       "n_subfeatures=0; max_depth=-1; min_samples_leaf=1; min_samples_split=2\n",
       "min_purity_increase=0.0; pruning_purity = 1.0; seed=3\n",
       "\n",
       "model    =   build_tree(labels, features,\n",
       "                        n_subfeatures,\n",
       "                        max_depth,\n",
       "                        min_samples_leaf,\n",
       "                        min_samples_split,\n",
       "                        min_purity_increase;\n",
       "                        rng = seed)\n",
       "\n",
       "accuracy = nfoldCV_tree(labels, features,\n",
       "                        n_folds,\n",
       "                        pruning_purity,\n",
       "                        max_depth,\n",
       "                        min_samples_leaf,\n",
       "                        min_samples_split,\n",
       "                        min_purity_increase;\n",
       "                        verbose = true,\n",
       "                        rng = seed)\n",
       "```\n",
       "\n",
       "Random Forest Classifier\n",
       "\n",
       "```julia\n",
       "# train random forest classifier\n",
       "# using 2 random features, 10 trees, 0.5 portion of samples per tree, and a maximum tree depth of 6\n",
       "model = build_forest(labels, features, 2, 10, 0.5, 6)\n",
       "# apply learned model\n",
       "apply_forest(model, [5.9,3.0,5.1,1.9])\n",
       "# get the probability of each label\n",
       "apply_forest_proba(model, [5.9,3.0,5.1,1.9], [\"Iris-setosa\", \"Iris-versicolor\", \"Iris-virginica\"])\n",
       "# run 3-fold cross validation for forests, using 2 random features per split\n",
       "n_folds=3; n_subfeatures=2\n",
       "accuracy = nfoldCV_forest(labels, features, n_folds, n_subfeatures)\n",
       "\n",
       "# set of classification parameters and respective default values\n",
       "# n_subfeatures: number of features to consider at random per split (default: -1, sqrt(# features))\n",
       "# n_trees: number of trees to train (default: 10)\n",
       "# partial_sampling: fraction of samples to train each tree on (default: 0.7)\n",
       "# max_depth: maximum depth of the decision trees (default: no maximum)\n",
       "# min_samples_leaf: the minimum number of samples each leaf needs to have (default: 5)\n",
       "# min_samples_split: the minimum number of samples in needed for a split (default: 2)\n",
       "# min_purity_increase: minimum purity needed for a split (default: 0.0)\n",
       "# keyword rng: the random number generator or seed to use (default Random.GLOBAL_RNG)\n",
       "#              multi-threaded forests must be seeded with an `Int`\n",
       "n_subfeatures=-1; n_trees=10; partial_sampling=0.7; max_depth=-1\n",
       "min_samples_leaf=5; min_samples_split=2; min_purity_increase=0.0; seed=3\n",
       "\n",
       "model    =   build_forest(labels, features,\n",
       "                          n_subfeatures,\n",
       "                          n_trees,\n",
       "                          partial_sampling,\n",
       "                          max_depth,\n",
       "                          min_samples_leaf,\n",
       "                          min_samples_split,\n",
       "                          min_purity_increase;\n",
       "                          rng = seed)\n",
       "\n",
       "accuracy = nfoldCV_forest(labels, features,\n",
       "                          n_folds,\n",
       "                          n_subfeatures,\n",
       "                          n_trees,\n",
       "                          partial_sampling,\n",
       "                          max_depth,\n",
       "                          min_samples_leaf,\n",
       "                          min_samples_split,\n",
       "                          min_purity_increase;\n",
       "                          verbose = true,\n",
       "                          rng = seed)\n",
       "```\n",
       "\n",
       "Adaptive-Boosted Decision Stumps Classifier\n",
       "\n",
       "```julia\n",
       "# train adaptive-boosted stumps, using 7 iterations\n",
       "model, coeffs = build_adaboost_stumps(labels, features, 7);\n",
       "# apply learned model\n",
       "apply_adaboost_stumps(model, coeffs, [5.9,3.0,5.1,1.9])\n",
       "# get the probability of each label\n",
       "apply_adaboost_stumps_proba(model, coeffs, [5.9,3.0,5.1,1.9], [\"Iris-setosa\", \"Iris-versicolor\", \"Iris-virginica\"])\n",
       "# run 3-fold cross validation for boosted stumps, using 7 iterations\n",
       "n_iterations=7; n_folds=3\n",
       "accuracy = nfoldCV_stumps(labels, features,\n",
       "                          n_folds,\n",
       "                          n_iterations;\n",
       "                          verbose = true)\n",
       "```\n",
       "\n",
       "### Regression Example\n",
       "\n",
       "```julia\n",
       "n, m = 10^3, 5\n",
       "features = randn(n, m)\n",
       "weights = rand(-2:2, m)\n",
       "labels = features * weights\n",
       "```\n",
       "\n",
       "Regression Tree ```julia\n",
       "\n",
       "[output truncated to first 200 lines]\n"
      ],
      "text/plain": [
       "  No docstring found for module \u001b[36mDecisionTree\u001b[39m.\n",
       "\n",
       "\u001b[1m  Exported names\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "  \u001b[36mAdaBoostStumpClassifier\u001b[39m, \u001b[36mConfusionMatrix\u001b[39m, \u001b[36mDecisionTreeClassifier\u001b[39m,\n",
       "  \u001b[36mDecisionTreeRegressor\u001b[39m, \u001b[36mEnsemble\u001b[39m, \u001b[36mLeaf\u001b[39m, \u001b[36mNode\u001b[39m, \u001b[36mR2\u001b[39m, \u001b[36mRandomForestClassifier\u001b[39m,\n",
       "  \u001b[36mRandomForestRegressor\u001b[39m, \u001b[36mapply_adaboost_stumps\u001b[39m, \u001b[36mapply_adaboost_stumps_proba\u001b[39m,\n",
       "  \u001b[36mapply_forest\u001b[39m, \u001b[36mapply_forest_proba\u001b[39m, \u001b[36mapply_tree\u001b[39m, \u001b[36mapply_tree_proba\u001b[39m,\n",
       "  \u001b[36mbuild_adaboost_stumps\u001b[39m, \u001b[36mbuild_forest\u001b[39m, \u001b[36mbuild_stump\u001b[39m, \u001b[36mbuild_tree\u001b[39m,\n",
       "  \u001b[36mconfusion_matrix\u001b[39m, \u001b[36mdepth\u001b[39m, \u001b[36mfit!\u001b[39m, \u001b[36mget_classes\u001b[39m, \u001b[36mload_data\u001b[39m, \u001b[36mmajority_vote\u001b[39m,\n",
       "  \u001b[36mmean_squared_error\u001b[39m, \u001b[36mnfoldCV_forest\u001b[39m, \u001b[36mnfoldCV_stumps\u001b[39m, \u001b[36mnfoldCV_tree\u001b[39m, \u001b[36mpredict\u001b[39m,\n",
       "  \u001b[36mpredict_proba\u001b[39m, \u001b[36mprint_tree\u001b[39m, \u001b[36mprune_tree\u001b[39m\n",
       "\n",
       "\u001b[1m  Displaying contents of readme found at\u001b[22m\n",
       "\u001b[1m \u001b[36m/Users/j1/.julia/packages/DecisionTree/iWCbW/README.md\u001b[39m\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "\u001b[1m  DecisionTree.jl\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "  (Image: CI)\n",
       "  (https://github.com/bensadeghi/DecisionTree.jl/actions?query=workflow%3ACI)\n",
       "  (Image: Codecov) (https://codecov.io/gh/bensadeghi/DecisionTree.jl) (Image:\n",
       "  Docs Stable) (https://juliahub.com/docs/DecisionTree/pEDeB/0.10.11/)\n",
       "\n",
       "  Julia implementation of Decision Tree (CART) and Random Forest algorithms\n",
       "\n",
       "  Available via:\n",
       "\n",
       "    •  AutoMLPipeline.jl (https://github.com/IBM/AutoMLPipeline.jl) -\n",
       "       create complex ML pipeline structures using simple expressions\n",
       "\n",
       "    •  CombineML.jl (https://github.com/ppalmes/CombineML.jl) - a\n",
       "       heterogeneous ensemble learning package\n",
       "\n",
       "    •  MLJ.jl (https://github.com/alan-turing-institute/MLJ.jl) - a\n",
       "       machine learning framework for Julia\n",
       "\n",
       "    •  ScikitLearn.jl (https://github.com/cstjean/ScikitLearn.jl) - Julia\n",
       "       implementation of the scikit-learn API\n",
       "\n",
       "\u001b[1m  Classification\u001b[22m\n",
       "\u001b[1m  ================\u001b[22m\n",
       "\n",
       "    •  pre-pruning (max depth, min leaf size)\n",
       "\n",
       "    •  post-pruning (pessimistic pruning)\n",
       "\n",
       "    •  multi-threaded bagging (random forests)\n",
       "\n",
       "    •  adaptive boosting (decision stumps)\n",
       "\n",
       "    •  cross validation (n-fold)\n",
       "\n",
       "    •  support for ordered features (encoded as \u001b[36mReal\u001b[39ms or \u001b[36mString\u001b[39ms)\n",
       "\n",
       "\u001b[1m  Regression\u001b[22m\n",
       "\u001b[1m  ============\u001b[22m\n",
       "\n",
       "    •  pre-pruning (max depth, min leaf size)\n",
       "\n",
       "    •  multi-threaded bagging (random forests)\n",
       "\n",
       "    •  cross validation (n-fold)\n",
       "\n",
       "    •  support for numerical features\n",
       "\n",
       "  \u001b[1mNote that regression is implied if labels/targets are of type Array{Float}\u001b[22m\n",
       "\n",
       "\u001b[1m  Installation\u001b[22m\n",
       "\u001b[1m  ==============\u001b[22m\n",
       "\n",
       "  You can install DecisionTree.jl using Julia's package manager\n",
       "\n",
       "\u001b[36m  Pkg.add(\"DecisionTree\")\u001b[39m\n",
       "\n",
       "\u001b[1m  ScikitLearn.jl API\u001b[22m\n",
       "\u001b[1m  ====================\u001b[22m\n",
       "\n",
       "  DecisionTree.jl supports the ScikitLearn.jl\n",
       "  (https://github.com/cstjean/ScikitLearn.jl) interface and algorithms\n",
       "  (cross-validation, hyperparameter tuning, pipelines, etc.)\n",
       "\n",
       "  Available models: \u001b[36mDecisionTreeClassifier, DecisionTreeRegressor,\n",
       "  RandomForestClassifier, RandomForestRegressor, AdaBoostStumpClassifier\u001b[39m. See\n",
       "  each model's help (eg. \u001b[36m?DecisionTreeRegressor\u001b[39m at the REPL) for more\n",
       "  information\n",
       "\n",
       "\u001b[1m  Classification Example\u001b[22m\n",
       "\u001b[1m  ––––––––––––––––––––––––\u001b[22m\n",
       "\n",
       "  Load DecisionTree package\n",
       "\n",
       "\u001b[36m  using DecisionTree\u001b[39m\n",
       "\n",
       "  Separate Fisher's Iris dataset features and labels\n",
       "\n",
       "\u001b[36m  features, labels = load_data(\"iris\")    # also see \"adult\" and \"digits\" datasets\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  # the data loaded are of type Array{Any}\u001b[39m\n",
       "\u001b[36m  # cast them to concrete types for better performance\u001b[39m\n",
       "\u001b[36m  features = float.(features)\u001b[39m\n",
       "\u001b[36m  labels   = string.(labels)\u001b[39m\n",
       "\n",
       "  Pruned Tree Classifier\n",
       "\n",
       "\u001b[36m  # train depth-truncated classifier\u001b[39m\n",
       "\u001b[36m  model = DecisionTreeClassifier(max_depth=2)\u001b[39m\n",
       "\u001b[36m  fit!(model, features, labels)\u001b[39m\n",
       "\u001b[36m  # pretty print of the tree, to a depth of 5 nodes (optional)\u001b[39m\n",
       "\u001b[36m  print_tree(model, 5)\u001b[39m\n",
       "\u001b[36m  # apply learned model\u001b[39m\n",
       "\u001b[36m  predict(model, [5.9,3.0,5.1,1.9])\u001b[39m\n",
       "\u001b[36m  # get the probability of each label\u001b[39m\n",
       "\u001b[36m  predict_proba(model, [5.9,3.0,5.1,1.9])\u001b[39m\n",
       "\u001b[36m  println(get_classes(model)) # returns the ordering of the columns in predict_proba's output\u001b[39m\n",
       "\u001b[36m  # run n-fold cross validation over 3 CV folds\u001b[39m\n",
       "\u001b[36m  # See ScikitLearn.jl for installation instructions\u001b[39m\n",
       "\u001b[36m  using ScikitLearn.CrossValidation: cross_val_score\u001b[39m\n",
       "\u001b[36m  accuracy = cross_val_score(model, features, labels, cv=3)\u001b[39m\n",
       "\n",
       "  Also, have a look at these classification\n",
       "  (https://github.com/cstjean/ScikitLearn.jl/blob/master/examples/Classifier_Comparison_Julia.ipynb)\n",
       "  and regression\n",
       "  (https://github.com/cstjean/ScikitLearn.jl/blob/master/examples/Decision_Tree_Regression_Julia.ipynb)\n",
       "  notebooks.\n",
       "\n",
       "\u001b[1m  Native API\u001b[22m\n",
       "\u001b[1m  ============\u001b[22m\n",
       "\n",
       "\u001b[1m  Classification Example\u001b[22m\n",
       "\u001b[1m  ––––––––––––––––––––––––\u001b[22m\n",
       "\n",
       "  Decision Tree Classifier\n",
       "\n",
       "\u001b[36m  # train full-tree classifier\u001b[39m\n",
       "\u001b[36m  model = build_tree(labels, features)\u001b[39m\n",
       "\u001b[36m  # prune tree: merge leaves having >= 90% combined purity (default: 100%)\u001b[39m\n",
       "\u001b[36m  model = prune_tree(model, 0.9)\u001b[39m\n",
       "\u001b[36m  # pretty print of the tree, to a depth of 5 nodes (optional)\u001b[39m\n",
       "\u001b[36m  print_tree(model, 5)\u001b[39m\n",
       "\u001b[36m  # apply learned model\u001b[39m\n",
       "\u001b[36m  apply_tree(model, [5.9,3.0,5.1,1.9])\u001b[39m\n",
       "\u001b[36m  # apply model to all the sames\u001b[39m\n",
       "\u001b[36m  preds = apply_tree(model, features)\u001b[39m\n",
       "\u001b[36m  # generate confusion matrix, along with accuracy and kappa scores\u001b[39m\n",
       "\u001b[36m  confusion_matrix(labels, preds)\u001b[39m\n",
       "\u001b[36m  # get the probability of each label\u001b[39m\n",
       "\u001b[36m  apply_tree_proba(model, [5.9,3.0,5.1,1.9], [\"Iris-setosa\", \"Iris-versicolor\", \"Iris-virginica\"])\u001b[39m\n",
       "\u001b[36m  # run 3-fold cross validation of pruned tree,\u001b[39m\n",
       "\u001b[36m  n_folds=3\u001b[39m\n",
       "\u001b[36m  accuracy = nfoldCV_tree(labels, features, n_folds)\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  # set of classification parameters and respective default values\u001b[39m\n",
       "\u001b[36m  # pruning_purity: purity threshold used for post-pruning (default: 1.0, no pruning)\u001b[39m\n",
       "\u001b[36m  # max_depth: maximum depth of the decision tree (default: -1, no maximum)\u001b[39m\n",
       "\u001b[36m  # min_samples_leaf: the minimum number of samples each leaf needs to have (default: 1)\u001b[39m\n",
       "\u001b[36m  # min_samples_split: the minimum number of samples in needed for a split (default: 2)\u001b[39m\n",
       "\u001b[36m  # min_purity_increase: minimum purity needed for a split (default: 0.0)\u001b[39m\n",
       "\u001b[36m  # n_subfeatures: number of features to select at random (default: 0, keep all)\u001b[39m\n",
       "\u001b[36m  # keyword rng: the random number generator or seed to use (default Random.GLOBAL_RNG)\u001b[39m\n",
       "\u001b[36m  n_subfeatures=0; max_depth=-1; min_samples_leaf=1; min_samples_split=2\u001b[39m\n",
       "\u001b[36m  min_purity_increase=0.0; pruning_purity = 1.0; seed=3\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  model    =   build_tree(labels, features,\u001b[39m\n",
       "\u001b[36m                          n_subfeatures,\u001b[39m\n",
       "\u001b[36m                          max_depth,\u001b[39m\n",
       "\u001b[36m                          min_samples_leaf,\u001b[39m\n",
       "\u001b[36m                          min_samples_split,\u001b[39m\n",
       "\u001b[36m                          min_purity_increase;\u001b[39m\n",
       "\u001b[36m                          rng = seed)\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  accuracy = nfoldCV_tree(labels, features,\u001b[39m\n",
       "\u001b[36m                          n_folds,\u001b[39m\n",
       "\u001b[36m                          pruning_purity,\u001b[39m\n",
       "\u001b[36m                          max_depth,\u001b[39m\n",
       "\u001b[36m                          min_samples_leaf,\u001b[39m\n",
       "\u001b[36m                          min_samples_split,\u001b[39m\n",
       "\u001b[36m                          min_purity_increase;\u001b[39m\n",
       "\u001b[36m                          verbose = true,\u001b[39m\n",
       "\u001b[36m                          rng = seed)\u001b[39m\n",
       "\n",
       "  Random Forest Classifier\n",
       "\n",
       "\u001b[36m  # train random forest classifier\u001b[39m\n",
       "\u001b[36m  # using 2 random features, 10 trees, 0.5 portion of samples per tree, and a maximum tree depth of 6\u001b[39m\n",
       "\u001b[36m  model = build_forest(labels, features, 2, 10, 0.5, 6)\u001b[39m\n",
       "\u001b[36m  # apply learned model\u001b[39m\n",
       "\u001b[36m  apply_forest(model, [5.9,3.0,5.1,1.9])\u001b[39m\n",
       "\u001b[36m  # get the probability of each label\u001b[39m\n",
       "\u001b[36m  apply_forest_proba(model, [5.9,3.0,5.1,1.9], [\"Iris-setosa\", \"Iris-versicolor\", \"Iris-virginica\"])\u001b[39m\n",
       "\u001b[36m  # run 3-fold cross validation for forests, using 2 random features per split\u001b[39m\n",
       "\u001b[36m  n_folds=3; n_subfeatures=2\u001b[39m\n",
       "\u001b[36m  accuracy = nfoldCV_forest(labels, features, n_folds, n_subfeatures)\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  # set of classification parameters and respective default values\u001b[39m\n",
       "\u001b[36m  # n_subfeatures: number of features to consider at random per split (default: -1, sqrt(# features))\u001b[39m\n",
       "\u001b[36m  # n_trees: number of trees to train (default: 10)\u001b[39m\n",
       "\u001b[36m  # partial_sampling: fraction of samples to train each tree on (default: 0.7)\u001b[39m\n",
       "\u001b[36m  # max_depth: maximum depth of the decision trees (default: no maximum)\u001b[39m\n",
       "\u001b[36m  # min_samples_leaf: the minimum number of samples each leaf needs to have (default: 5)\u001b[39m\n",
       "\u001b[36m  # min_samples_split: the minimum number of samples in needed for a split (default: 2)\u001b[39m\n",
       "\u001b[36m  # min_purity_increase: minimum purity needed for a split (default: 0.0)\u001b[39m\n",
       "\u001b[36m  # keyword rng: the random number generator or seed to use (default Random.GLOBAL_RNG)\u001b[39m\n",
       "\u001b[36m  #              multi-threaded forests must be seeded with an `Int`\u001b[39m\n",
       "\u001b[36m  n_subfeatures=-1; n_trees=10; partial_sampling=0.7; max_depth=-1\u001b[39m\n",
       "\u001b[36m  min_samples_leaf=5; min_samples_split=2; min_purity_increase=0.0; seed=3\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  model    =   build_forest(labels, features,\u001b[39m\n",
       "\u001b[36m                            n_subfeatures,\u001b[39m\n",
       "\u001b[36m                            n_trees,\u001b[39m\n",
       "\u001b[36m                            partial_sampling,\u001b[39m\n",
       "\u001b[36m                            max_depth,\u001b[39m\n",
       "\u001b[36m                            min_samples_leaf,\u001b[39m\n",
       "\u001b[36m                            min_samples_split,\u001b[39m\n",
       "\u001b[36m                            min_purity_increase;\u001b[39m\n",
       "\u001b[36m                            rng = seed)\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  accuracy = nfoldCV_forest(labels, features,\u001b[39m\n",
       "\u001b[36m                            n_folds,\u001b[39m\n",
       "\u001b[36m                            n_subfeatures,\u001b[39m\n",
       "\u001b[36m                            n_trees,\u001b[39m\n",
       "\u001b[36m                            partial_sampling,\u001b[39m\n",
       "\u001b[36m                            max_depth,\u001b[39m\n",
       "\u001b[36m                            min_samples_leaf,\u001b[39m\n",
       "\u001b[36m                            min_samples_split,\u001b[39m\n",
       "\u001b[36m                            min_purity_increase;\u001b[39m\n",
       "\u001b[36m                            verbose = true,\u001b[39m\n",
       "\u001b[36m                            rng = seed)\u001b[39m\n",
       "\n",
       "  Adaptive-Boosted Decision Stumps Classifier\n",
       "\n",
       "\u001b[36m  # train adaptive-boosted stumps, using 7 iterations\u001b[39m\n",
       "\u001b[36m  model, coeffs = build_adaboost_stumps(labels, features, 7);\u001b[39m\n",
       "\u001b[36m  # apply learned model\u001b[39m\n",
       "\u001b[36m  apply_adaboost_stumps(model, coeffs, [5.9,3.0,5.1,1.9])\u001b[39m\n",
       "\u001b[36m  # get the probability of each label\u001b[39m\n",
       "\u001b[36m  apply_adaboost_stumps_proba(model, coeffs, [5.9,3.0,5.1,1.9], [\"Iris-setosa\", \"Iris-versicolor\", \"Iris-virginica\"])\u001b[39m\n",
       "\u001b[36m  # run 3-fold cross validation for boosted stumps, using 7 iterations\u001b[39m\n",
       "\u001b[36m  n_iterations=7; n_folds=3\u001b[39m\n",
       "\u001b[36m  accuracy = nfoldCV_stumps(labels, features,\u001b[39m\n",
       "\u001b[36m                            n_folds,\u001b[39m\n",
       "\u001b[36m                            n_iterations;\u001b[39m\n",
       "\u001b[36m                            verbose = true)\u001b[39m\n",
       "\n",
       "\u001b[1m  Regression Example\u001b[22m\n",
       "\u001b[1m  ––––––––––––––––––––\u001b[22m\n",
       "\n",
       "\u001b[36m  n, m = 10^3, 5\u001b[39m\n",
       "\u001b[36m  features = randn(n, m)\u001b[39m\n",
       "\u001b[36m  weights = rand(-2:2, m)\u001b[39m\n",
       "\u001b[36m  labels = features * weights\u001b[39m\n",
       "\n",
       "  Regression Tree ```julia\n",
       "\n",
       "  [output truncated to first 200 lines]"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?DecisionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "72a88283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier\n",
       "max_depth:                -1\n",
       "min_samples_leaf:         1\n",
       "min_samples_split:        2\n",
       "min_purity_increase:      0.0\n",
       "pruning_purity_threshold: 1.0\n",
       "n_subfeatures:            0\n",
       "classes:                  String1[\"M\", \"R\"]\n",
       "root:                     Decision Tree\n",
       "Leaves: 14\n",
       "Depth:  6"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelDT=DecisionTreeClassifier()\n",
    "DecisionTree.fit!(modelDT, Xtraining, ytraining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cbe2966f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 12, Threshold 0.22875\n",
      "L-> Feature 21, Threshold 0.80055\n",
      "    L-> Feature 54, Threshold 0.0012000000000000001\n",
      "        L-> M : 1/1\n",
      "        R-> R : 45/45\n",
      "    R-> Feature 42, Threshold 0.08524999999999999\n",
      "        L-> R : 5/5\n",
      "        R-> Feature 51, Threshold 0.0066\n",
      "            L-> R : 1/1\n",
      "            R-> M : 9/9\n",
      "R-> Feature 51, Threshold 0.013600000000000001\n",
      "    L-> Feature 27, Threshold 0.9056500000000001\n",
      "        L-> Feature 50, Threshold 0.00635\n",
      "            L-> M : 3/3\n",
      "            R-> Feature 23, Threshold 0.83135\n",
      "                L-> R : 15/15\n",
      "                R-> Feature 22, Threshold 0.7843\n",
      "                    L-> R : 2/2\n",
      "                    R-> M : 3/3\n",
      "        R-> M : 9/9\n",
      "    R-> Feature 34, Threshold 0.87495\n",
      "        L-> Feature 22, Threshold 0.8881\n",
      "            L-> M : 40/40\n",
      "            R-> Feature 33, Threshold 0.2353\n",
      "                L-> M : 3/3\n",
      "                R-> R : 2/2\n",
      "        R-> R : 2/2\n"
     ]
    }
   ],
   "source": [
    "####print tree model\n",
    "\n",
    "print_tree(modelDT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "eeae3359",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67-element Vector{String1}:\n",
       " \"M\"\n",
       " \"R\"\n",
       " \"M\"\n",
       " \"R\"\n",
       " \"R\"\n",
       " \"M\"\n",
       " \"M\"\n",
       " \"R\"\n",
       " \"M\"\n",
       " \"R\"\n",
       " \"R\"\n",
       " \"R\"\n",
       " \"R\"\n",
       " ⋮\n",
       " \"M\"\n",
       " \"M\"\n",
       " \"M\"\n",
       " \"M\"\n",
       " \"R\"\n",
       " \"M\"\n",
       " \"M\"\n",
       " \"M\"\n",
       " \"M\"\n",
       " \"M\"\n",
       " \"M\"\n",
       " \"M\""
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhatDT=DecisionTree.predict(modelDT, Xtesting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f8bf0f",
   "metadata": {},
   "source": [
    "### What's the best Tree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0c836e94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7164179104477612"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#####accuracy\n",
    "DTaccuracy=findaccuracy(yhatDT, ytesting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9e6505d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67×3 Matrix{Any}:\n",
       " \"M\"  \"R\"  false\n",
       " \"R\"  \"R\"   true\n",
       " \"M\"  \"R\"  false\n",
       " \"R\"  \"R\"   true\n",
       " \"R\"  \"R\"   true\n",
       " \"M\"  \"R\"  false\n",
       " \"M\"  \"R\"  false\n",
       " \"R\"  \"R\"   true\n",
       " \"M\"  \"R\"  false\n",
       " \"R\"  \"R\"   true\n",
       " \"R\"  \"R\"   true\n",
       " \"R\"  \"R\"   true\n",
       " \"R\"  \"R\"   true\n",
       " ⋮         \n",
       " \"M\"  \"M\"   true\n",
       " \"M\"  \"M\"   true\n",
       " \"M\"  \"M\"   true\n",
       " \"M\"  \"M\"   true\n",
       " \"R\"  \"M\"  false\n",
       " \"M\"  \"M\"   true\n",
       " \"M\"  \"M\"   true\n",
       " \"M\"  \"M\"   true\n",
       " \"M\"  \"M\"   true\n",
       " \"M\"  \"M\"   true\n",
       " \"M\"  \"M\"   true\n",
       " \"M\"  \"M\"   true"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###check on screen the results\n",
    "checkDT=[yhatDT[i] == ytesting[i] for i in 1:length(yhatDT)]\n",
    "checkDTdisplay=[yhatDT ytesting checkDT]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a9d2ec",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24324977",
   "metadata": {},
   "source": [
    "We need to use both, the k-NN, and k-DTree models,  because our feauteres are higtly dimentionality. In the case of k-NN, 'k' represents how many neighbors we want. but, k-DTree, 'k' represents the dimension for our data. k-Dtree put our data in a euclidian space. (https://es.wikipedia.org/wiki/Árbol_kd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d258274d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1mK\u001b[22m\u001b[0m\u001b[1mD\u001b[22m\u001b[0m\u001b[1mT\u001b[22m\u001b[0m\u001b[1mr\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1me\u001b[22m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "KDTree(data [, metric = Euclidean(); leafsize = 10, reorder = true]) -> kdtree\n",
       "\\end{verbatim}\n",
       "Creates a \\texttt{KDTree} from the data using the given \\texttt{metric} and \\texttt{leafsize}. The \\texttt{metric} must be a \\texttt{MinkowskiMetric}.\n",
       "\n"
      ],
      "text/markdown": [
       "```\n",
       "KDTree(data [, metric = Euclidean(); leafsize = 10, reorder = true]) -> kdtree\n",
       "```\n",
       "\n",
       "Creates a `KDTree` from the data using the given `metric` and `leafsize`. The `metric` must be a `MinkowskiMetric`.\n"
      ],
      "text/plain": [
       "\u001b[36m  KDTree(data [, metric = Euclidean(); leafsize = 10, reorder = true]) -> kdtree\u001b[39m\n",
       "\n",
       "  Creates a \u001b[36mKDTree\u001b[39m from the data using the given \u001b[36mmetric\u001b[39m and \u001b[36mleafsize\u001b[39m. The\n",
       "  \u001b[36mmetric\u001b[39m must be a \u001b[36mMinkowskiMetric\u001b[39m."
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?KDTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "29fc487a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KDTree{StaticArrays.SVector{60, Float64}, Euclidean, Float64}\n",
       "  Number of points: 140\n",
       "  Dimensions: 60\n",
       "  Metric: Euclidean(0.0)\n",
       "  Reordered: true"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### let's to crate our model\n",
    "kdtree=KDTree(Xtraining')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4a21f26a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1mk\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1mn\u001b[22m bac\u001b[0m\u001b[1mk\u001b[22me\u001b[0m\u001b[1mn\u001b[22md_\u001b[0m\u001b[1mn\u001b[22mame Ran\u001b[0m\u001b[1mk\u001b[22mDeficie\u001b[0m\u001b[1mn\u001b[22mtExceptio\u001b[0m\u001b[1mn\u001b[22m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "knn(tree::NNTree, points, k [, sortres=false]) -> indices, distances\n",
       "nn(tree:NNTree, points) -> indices, distances\n",
       "\\end{verbatim}\n",
       "Performs a lookup of the \\texttt{k} nearest neigbours to the \\texttt{points} from the data in the \\texttt{tree}. If \\texttt{sortres = true} the result is sorted such that the results are in the order of increasing distance to the point. \\texttt{skip} is an optional predicate to determine if a point that would be returned should be skipped based on its  index.\n",
       "\n"
      ],
      "text/markdown": [
       "```\n",
       "knn(tree::NNTree, points, k [, sortres=false]) -> indices, distances\n",
       "nn(tree:NNTree, points) -> indices, distances\n",
       "```\n",
       "\n",
       "Performs a lookup of the `k` nearest neigbours to the `points` from the data in the `tree`. If `sortres = true` the result is sorted such that the results are in the order of increasing distance to the point. `skip` is an optional predicate to determine if a point that would be returned should be skipped based on its  index.\n"
      ],
      "text/plain": [
       "\u001b[36m  knn(tree::NNTree, points, k [, sortres=false]) -> indices, distances\u001b[39m\n",
       "\u001b[36m  nn(tree:NNTree, points) -> indices, distances\u001b[39m\n",
       "\n",
       "  Performs a lookup of the \u001b[36mk\u001b[39m nearest neigbours to the \u001b[36mpoints\u001b[39m from the data in\n",
       "  the \u001b[36mtree\u001b[39m. If \u001b[36msortres = true\u001b[39m the result is sorted such that the results are\n",
       "  in the order of increasing distance to the point. \u001b[36mskip\u001b[39m is an optional\n",
       "  predicate to determine if a point that would be returned should be skipped\n",
       "  based on its index."
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "114401fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[92, 74, 73], [33, 16, 35], [129, 127, 132], [139, 140, 36], [27, 44, 48], [35, 28, 102], [85, 36, 37], [104, 36, 23], [78, 8, 6], [108, 77, 22]  …  [125, 124, 123], [125, 123, 126], [99, 124, 127], [39, 38, 139], [135, 134, 136], [135, 134, 136], [134, 135, 136], [134, 136, 135], [137, 138, 139], [138, 137, 139]], [[0.9041257821785641, 0.9087654923026075, 0.9915993192817347], [1.417511647218463, 1.455762751962008, 1.463766081038907], [1.3085757945186056, 1.3254247055189516, 1.3447413468767886], [0.6260336732157463, 0.6492787074900885, 0.7226501712447037], [0.8840999830335934, 0.9227258910424048, 0.9307632244561449], [0.9379003251945273, 1.1440832836817432, 1.1537823408251662], [1.0913069458223017, 1.1247289584606592, 1.1284789098605255], [1.2227029034070378, 1.2323320088352814, 1.2339161397761194], [1.1139421932937097, 1.1573166766274474, 1.1871245090553897], [1.2818515124615641, 1.2857210000618329, 1.294053893777226]  …  [0.3540287982636441, 0.6080993668801178, 0.7334335348209815], [0.40255443110218025, 0.46694441853394075, 0.6332001579279652], [0.9418612318170866, 0.9683696814750035, 1.1178768089552622], [0.5303065434255928, 0.5600932065290561, 0.5686655080097615], [0.26048282093067093, 0.3598298764694226, 0.5126416974847053], [0.19799747473137128, 0.3087104144663734, 0.4392977236453656], [0.3047025434747797, 0.32839287446593596, 0.345157369905381], [0.3219759928938802, 0.34779023850591323, 0.3568348357433729], [0.4287572040211104, 0.492501888321253, 0.5233976595285845], [0.2825406873354703, 0.5137034066462867, 0.6671874549180311]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k=3\n",
    "indecesknn, distances=knn(kdtree,Xtesting', k, true )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1750163e",
   "metadata": {},
   "source": [
    "This results certanly are diffciult to intepretet, because we need to processing this output like clasiffications:\n",
    "SO, this first stpe is convert the lis of kNN into a vector of vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5e6c22b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67×3 adjoint(::Matrix{Int64}) with eltype Int64:\n",
       "  92   74   73\n",
       "  33   16   35\n",
       " 129  127  132\n",
       " 139  140   36\n",
       "  27   44   48\n",
       "  35   28  102\n",
       "  85   36   37\n",
       " 104   36   23\n",
       "  78    8    6\n",
       " 108   77   22\n",
       "  40   28   27\n",
       "  27  122   48\n",
       "  17   44   48\n",
       "   ⋮       \n",
       " 120  123  125\n",
       "  42   41   38\n",
       " 125  124  123\n",
       " 125  123  126\n",
       "  99  124  127\n",
       "  39   38  139\n",
       " 135  134  136\n",
       " 135  134  136\n",
       " 134  135  136\n",
       " 134  136  135\n",
       " 137  138  139\n",
       " 138  137  139"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###porcessing \n",
    "using StatsBase\n",
    "indeces_kNNMatrix=hcat(indecesknn...)'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe51e65",
   "metadata": {},
   "source": [
    "In this matrix we can see, in every row the kNN'positions for every point in the testing data. For example, in the first row, we got that 102, 58 and 72 indeces are 3-NN for the first data testing. But we want to see this matrix better way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a2bb96ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67×3 PooledArrays.PooledMatrix{String1, UInt32, Matrix{UInt32}}:\n",
       " \"M\"  \"M\"  \"M\"\n",
       " \"R\"  \"R\"  \"R\"\n",
       " \"M\"  \"M\"  \"M\"\n",
       " \"M\"  \"M\"  \"R\"\n",
       " \"R\"  \"R\"  \"R\"\n",
       " \"R\"  \"R\"  \"M\"\n",
       " \"M\"  \"R\"  \"R\"\n",
       " \"M\"  \"R\"  \"R\"\n",
       " \"M\"  \"R\"  \"R\"\n",
       " \"M\"  \"M\"  \"R\"\n",
       " \"R\"  \"R\"  \"R\"\n",
       " \"R\"  \"M\"  \"R\"\n",
       " \"R\"  \"R\"  \"R\"\n",
       " ⋮         \n",
       " \"M\"  \"M\"  \"M\"\n",
       " \"R\"  \"R\"  \"R\"\n",
       " \"M\"  \"M\"  \"M\"\n",
       " \"M\"  \"M\"  \"M\"\n",
       " \"M\"  \"M\"  \"M\"\n",
       " \"R\"  \"R\"  \"M\"\n",
       " \"M\"  \"M\"  \"M\"\n",
       " \"M\"  \"M\"  \"M\"\n",
       " \"M\"  \"M\"  \"M\"\n",
       " \"M\"  \"M\"  \"M\"\n",
       " \"M\"  \"M\"  \"M\"\n",
       " \"M\"  \"M\"  \"M\""
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knnclasses=ytraining[indeces_kNNMatrix]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fc8e53",
   "metadata": {},
   "source": [
    "Now, this matrix is easier to read. For example, in the last row, we can see that the last input on the testing data vector, has 3-NN that are mines, so this input will be classifeicadted like Mine. But again, if we see the first input, this has 3-NN, but two of them are 'Mines', and one 'Rock', so we want this input  be clasificated like Mine, because is the most popular label in the local neiborhood. With this propose, will be to use a funtion from StatsBase pkg: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e7953c",
   "metadata": {},
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "753d1233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{String1, Int64} with 1 entry:\n",
       "  \"M\" => 3"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countmap(knnclasses[1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7fa22f49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"M\""
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "argmax(countmap(knnclasses[1,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "771312f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"R\""
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "argmax(countmap(knnclasses[2,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "dbab598c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67-element Vector{String}:\n",
       " \"M\"\n",
       " \"R\"\n",
       " \"M\"\n",
       " \"M\"\n",
       " \"R\"\n",
       " \"R\"\n",
       " \"R\"\n",
       " \"R\"\n",
       " \"R\"\n",
       " \"M\"\n",
       " \"R\"\n",
       " \"R\"\n",
       " \"R\"\n",
       " ⋮\n",
       " \"M\"\n",
       " \"R\"\n",
       " \"M\"\n",
       " \"M\"\n",
       " \"M\"\n",
       " \"R\"\n",
       " \"M\"\n",
       " \"M\"\n",
       " \"M\"\n",
       " \"M\"\n",
       " \"M\"\n",
       " \"M\""
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhatknn=map(i-> string(argmax(countmap(knnclasses[i,:]))), 1:length(ytesting) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c6d235",
   "metadata": {},
   "source": [
    "### What's the best k-NN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "afe82a8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8507462686567164"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Knnaccuracy=findaccuracy(yhatknn,ytesting)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "339522cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67×3 Matrix{Any}:\n",
       " \"M\"  \"R\"  false\n",
       " \"R\"  \"R\"   true\n",
       " \"M\"  \"R\"  false\n",
       " \"M\"  \"R\"  false\n",
       " \"R\"  \"R\"   true\n",
       " \"R\"  \"R\"   true\n",
       " \"R\"  \"R\"   true\n",
       " \"R\"  \"R\"   true\n",
       " \"R\"  \"R\"   true\n",
       " \"M\"  \"R\"  false\n",
       " \"R\"  \"R\"   true\n",
       " \"R\"  \"R\"   true\n",
       " \"R\"  \"R\"   true\n",
       " ⋮         \n",
       " \"M\"  \"M\"   true\n",
       " \"R\"  \"M\"  false\n",
       " \"M\"  \"M\"   true\n",
       " \"M\"  \"M\"   true\n",
       " \"M\"  \"M\"   true\n",
       " \"R\"  \"M\"  false\n",
       " \"M\"  \"M\"   true\n",
       " \"M\"  \"M\"   true\n",
       " \"M\"  \"M\"   true\n",
       " \"M\"  \"M\"   true\n",
       " \"M\"  \"M\"   true\n",
       " \"M\"  \"M\"   true"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###check on screen the results\n",
    "checkKnn=[yhatknn[i] == ytesting[i] for i in 1:length(yhatknn)]\n",
    "checkDTdisplay=[yhatknn ytesting checkKnn]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "536944bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "#######################################################################################\n",
    "######################################## Logistic Regression #########################################################\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91d94fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ffe537",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "32d57882",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################################################\n",
    "###########\n",
    "############################ Linear Regression #########################################################\n",
    "\n",
    "\n",
    "using LaTeXStrings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a64ccc",
   "metadata": {},
   "source": [
    "Our feature vectors are higtly dimensional, so we want to generate the optimal hyperplane on  the data training:\n",
    "$$ f(X)=WX+b \\in \\mathbb{R} $$ \n",
    "So, let's to define our parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "7f80b754",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67-element Vector{Int64}:\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " ⋮\n",
       " 2\n",
       " 2\n",
       " 2\n",
       " 2\n",
       " 2\n",
       " 2\n",
       " 2\n",
       " 2\n",
       " 2\n",
       " 2\n",
       " 2\n",
       " 2"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sonarlabelsmap=labelmap(ytraining); #The function  'labelmap'\n",
    "#find how manny labels and which are\n",
    "ytraininglabels=labelencode(sonarlabelsmap, ytraining) # this function transform in anumerical label our stringslabels\n",
    "sonarlabelsmap2=labelmap(ytesting); #The function  'labelmap'\n",
    "#find how manny labels and which are\n",
    "ytestinglabels=labelencode(sonarlabelsmap2, ytesting) # this function transform in anumerical label our stringslabels\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "0e57f4aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×140 adjoint(::Vector{Float64}) with eltype Float64:\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####some tests\n",
    "\n",
    "\n",
    "#for i in:length(ytraining)\n",
    "#end\n",
    "\n",
    "W= zeros(size(Xtraining))[1,:]'\n",
    "b=zeros(size(Xtraining))[:,1]'\n",
    "#W*Xtraining[1,:]' .+ b\n",
    "\n",
    "    \n",
    "\n",
    "h(X)=W*X'+b\n",
    "yhatreg=h(Xtraining);\n",
    "#W*Xtraining'+b\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "d6d79e66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "regCost (generic function with 1 method)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N=length(ytraining)\n",
    "function regCost(X,y)\n",
    "   \n",
    "    (1/N)*sum((yhatreg-ytraininglabels').^2)\n",
    "end\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "f72cb57c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.457142857142857"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J=regCost(Xtraining,ytraininglabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "c341b28b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Vector{Any}:\n",
       " 2.457142857142857"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J_history=[]\n",
    "push!(J_history,J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "9ce869d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.0e-7"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####Partial derivate for Gradient decent algorithm\n",
    "function pd_b(X,Y)\n",
    "    (1/N)*sum(yhatreg-Y')\n",
    "end\n",
    "\n",
    "function pd_W(X,Y)\n",
    "    (1/N)*sum((yhatreg-Y').*X')\n",
    "end\n",
    "###some hyperparameters\n",
    "alpha0=0.09\n",
    "alpha1=0.0000009"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "03095408",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-25.235702142857143"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####begin iterations until convergence\n",
    "\n",
    "b0_temp=pd_b(Xtraining,ytraininglabels)\n",
    "W0_temp=pd_W(Xtraining,ytraininglabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "7f96cd62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.542426385714286e-5"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b0 -= alpha0*b0_temp\n",
    "W0 -= alpha1*W0_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "dbbcfa8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: Color\u001b[0m\u001b[1mG\u001b[22m\u001b[0m\u001b[1mr\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1md\u001b[22m\u001b[0m\u001b[1mi\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1mt\u001b[22m \u001b[0m\u001b[1mG\u001b[22mene\u001b[0m\u001b[1mr\u001b[22m\u001b[0m\u001b[1ma\u001b[22mlize\u001b[0m\u001b[1md\u001b[22mE\u001b[0m\u001b[1mi\u001b[22mg\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\n",
      "\n",
      "Couldn't find \u001b[36mgradient\u001b[39m\n",
      "Perhaps you meant green\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "No documentation found.\n",
       "\n",
       "Binding \\texttt{gradient} does not exist.\n",
       "\n"
      ],
      "text/markdown": [
       "No documentation found.\n",
       "\n",
       "Binding `gradient` does not exist.\n"
      ],
      "text/plain": [
       "  No documentation found.\n",
       "\n",
       "  Binding \u001b[36mgradient\u001b[39m does not exist."
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?gradient"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1590132f",
   "metadata": {
    "raw_mimetype": "text/latex"
   },
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Julia 1.7.2",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
